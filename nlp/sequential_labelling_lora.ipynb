{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sequential Labelling"]},{"cell_type":"markdown","metadata":{},"source":["## Import Modules"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:52:56.506710Z","iopub.status.busy":"2023-10-23T20:52:56.506311Z","iopub.status.idle":"2023-10-23T20:53:36.813051Z","shell.execute_reply":"2023-10-23T20:53:36.812261Z","shell.execute_reply.started":"2023-10-23T20:52:56.506674Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nusacrowd\n","  Downloading nusacrowd-0.1.2-py3-none-any.whl (384 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.2/384.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n","Collecting loguru>=0.5.3 (from nusacrowd)\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bioc>=1.3.7 (from nusacrowd)\n","  Downloading bioc-2.1-py3-none-any.whl (33 kB)\n","Collecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black~=22.0 (from nusacrowd)\n","  Downloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flake8>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (6.0.0)\n","Requirement already satisfied: isort>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (5.12.0)\n","Requirement already satisfied: aiohttp>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (3.8.4)\n","Collecting pre-commit>=2.19.0 (from nusacrowd)\n","  Downloading pre_commit-3.5.0-py2.py3-none-any.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonlines>=3.1.0 (from nusacrowd)\n","  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: torchaudio>=0.11 in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (2.0.1)\n","Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (0.12.1)\n","Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (0.10.1)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (3.2.4)\n","Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (0.19.0)\n","Collecting ffmpeg (from nusacrowd)\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting conllu (from nusacrowd)\n","  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (3.1.2)\n","Collecting translate-toolkit>=3.7.3 (from nusacrowd)\n","  Downloading translate_toolkit-3.10.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from nusacrowd) (4.6.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.8.1->nusacrowd) (1.3.1)\n","Requirement already satisfied: lxml>=4.6.3 in /opt/conda/lib/python3.10/site-packages (from bioc>=1.3.7->nusacrowd) (4.9.3)\n","Collecting intervaltree (from bioc>=1.3.7->nusacrowd)\n","  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from bioc>=1.3.7->nusacrowd) (0.6.2)\n","Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black~=22.0->nusacrowd) (8.1.7)\n","Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black~=22.0->nusacrowd) (1.0.0)\n","Collecting pathspec>=0.9.0 (from black~=22.0->nusacrowd)\n","  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n","Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black~=22.0->nusacrowd) (3.10.0)\n","Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black~=22.0->nusacrowd) (2.0.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from flake8>=3.8.3->nusacrowd) (0.7.0)\n","Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from flake8>=3.8.3->nusacrowd) (2.10.0)\n","Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flake8>=3.8.3->nusacrowd) (3.0.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n","Collecting cfgv>=2.0.0 (from pre-commit>=2.19.0->nusacrowd)\n","  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n","Collecting identify>=1.0.0 (from pre-commit>=2.19.0->nusacrowd)\n","  Downloading identify-2.5.30-py2.py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit>=2.19.0->nusacrowd)\n","  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit>=2.19.0->nusacrowd) (20.21.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchaudio>=0.11->nusacrowd) (2.0.0)\n","Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (3.0.0)\n","Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (0.57.1)\n","Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (1.7.0)\n","Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (0.3.6)\n","Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (0.2)\n","Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->nusacrowd) (1.0.5)\n","Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile->nusacrowd) (1.15.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->nusacrowd) (1.16.0)\n","Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->nusacrowd) (1.1.0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile->nusacrowd) (2.21)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nodeenv>=0.11.1->pre-commit>=2.19.0->nusacrowd) (68.0.0)\n","Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->nusacrowd) (0.40.1)\n","Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit>=2.19.0->nusacrowd) (0.3.6)\n","Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from intervaltree->bioc>=1.3.7->nusacrowd) (2.4.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio>=0.11->nusacrowd) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio>=0.11->nusacrowd) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio>=0.11->nusacrowd) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchaudio>=0.11->nusacrowd) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchaudio>=0.11->nusacrowd) (1.3.0)\n","Building wheels for collected packages: seqeval, ffmpeg, intervaltree\n","  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=6f27b445f3eb4ce33932c1ded8588fa44ebfae097b09dcea98b13fd15cfb5126\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=79a33a16b86ca17bc793850d280b698cff3cd78511f6b6707e9ca4907c8b8f7e\n","  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n","  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26099 sha256=360140cca1e76e8f504428af80575552a3f5dafc36386e5e50507d4e2662365a\n","  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n","Successfully built seqeval ffmpeg intervaltree\n","Installing collected packages: ffmpeg, translate-toolkit, pathspec, nodeenv, loguru, jsonlines, intervaltree, identify, conllu, cfgv, pre-commit, black, bioc, seqeval, datasets, nusacrowd, evaluate\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","Successfully installed bioc-2.1 black-22.12.0 cfgv-3.4.0 conllu-4.5.3 datasets-2.14.6 evaluate-0.4.1 ffmpeg-1.4 identify-2.5.30 intervaltree-3.1.0 jsonlines-4.0.0 loguru-0.7.2 nodeenv-1.8.0 nusacrowd-0.1.2 pathspec-0.11.2 pre-commit-3.5.0 seqeval-1.2.2 translate-toolkit-3.10.1\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["!pip install evaluate nusacrowd seqeval\n","import evaluate\n","import numpy as np\n","import transformers\n","import tensorflow as tf\n","\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorForTokenClassification, create_optimizer, TFAutoModelForTokenClassification\n","from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback"]},{"cell_type":"markdown","metadata":{},"source":["## Import Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:36.814531Z","iopub.status.busy":"2023-10-23T20:53:36.814230Z","iopub.status.idle":"2023-10-23T20:53:44.350336Z","shell.execute_reply":"2023-10-23T20:53:44.349449Z","shell.execute_reply.started":"2023-10-23T20:53:36.814505Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d11eb25ff4d433caa6c7195c27c032f","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.33k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea2994d543174831ac9f67d676d4d135","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a429e4ea7f164202bc7df2739d05fd09","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c59512602b594c1eb0fd060723b28a9e","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"630d7cef29d14dbaa9bcf155d149fbbf","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb43d6b11df84b39885bbdace5e2581d","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["nergrit = load_dataset('NusaCrowd/nergrit')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:44.352760Z","iopub.status.busy":"2023-10-23T20:53:44.352464Z","iopub.status.idle":"2023-10-23T20:53:44.359695Z","shell.execute_reply":"2023-10-23T20:53:44.358742Z","shell.execute_reply.started":"2023-10-23T20:53:44.352734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'index': '0', 'tokens': ['Indonesia', 'mengekspor', 'produk', 'industri', 'skala', 'besar', 'ke', 'Amerika', 'Serikat', '.', 'Ekspor', 'dilakukan', 'melalui', 'Pelabuhan', 'Tanjung', 'Priok', ',', 'Jakarta', 'Utara', ',', 'Selasa', '(', '15', '/', '8', '/', '2018', ')', '.', 'Komoditas', 'yang', 'dikirim', 'terdiri', 'dari', '50', 'persen', 'sepatu', ',', '15', 'persen', 'garmen', ',', '10', 'persen', 'produk', 'karet', ',', 'ban', 'dan', 'turunannya', ',', 'alat', '-', 'alat', 'elektronik', '10', 'persen', ',', 'dan', 'produk', 'lainnya', '15', 'persen', '.', '\"', 'Bukan', 'bahan', 'mentah', ',', 'tetapi', 'sudah', 'bahan', '-', 'bahan', 'produksi', ',', 'produk', '-', 'produk', 'industri', 'yang', 'kita', 'harapkan', 'ini', 'akan', 'meningkatkan', 'ekspor', 'kita', ',', '\"', 'kata', 'Presiden', 'dalam', 'sambutannya', 'pada', 'acara', 'pelepasan', 'ekspor', 'di', 'Jakarta', 'International', 'Container', 'Terminal', '(', 'JICT', ')', ',', 'Selasa', '(', '15', '/', '5', '/', '2018', ')', '.', 'Ekspor', 'tersebut', 'menggunakan', 'Kapal', 'MV', 'CMA', 'CGM', 'Tage', '.', 'Kapal', 'ini', 'merupakan', 'kapal', 'besar', 'berkapasitas', '10', 'ribu', 'TEUs', 'dan', 'berbobot', '95.263', 'GT', '(', 'Gross', 'Tonnage', ')', '.', 'Kapal', 'ini', 'memiliki', 'layanan', 'Java', '-', 'America', 'Express', '(', 'JAX', ')', 'yang', 'rutin', 'melayani', 'rute', 'Pelabuhan', 'Tanjung', 'Priok', 'ke', 'West', 'Coast', '(', 'LA', 'dan', 'Oakland', ')', 'Amerika', 'Serikat', '(', 'direct', 'call', ')', '.', '\"', 'Artinya', 'pengiriman', 'ini', 'besar', 'sekali', 'dan', 'dilakukan', 'dengan', 'sangat', 'efisien', 'dengan', 'direct', 'call', '.', 'Ini', 'akan', 'menurunkan', 'biaya', 'logistik', 'yang', 'sangat', 'besar', '.', 'Tadi', 'sudah', 'disampaikan', 'oleh', 'Dirut', 'Pelindo', 'bahwa', 'setiap', 'kontainer', 'menghemat', 'biaya', 'kurang', 'lebih', 'USD', '300', 'dan', 'ini', 'akan', 'memberikan', 'daya', 'saing', 'produk', '-', 'produk', 'kita', 'terhadap', 'produk', '-', 'produk', 'dari', 'negara', 'lain', ',', '\"', 'lanjutnya', '.', 'Presiden', 'juga', 'menyampaikan', 'bahwa', 'ekspor', 'ke', 'Amerika', 'Serikat', 'ini', 'menandakan', 'bahwa', 'Indonesia', 'memiliki', 'peran', 'yang', 'sangat', 'strategis', 'dalam', 'geo', 'ekonomi', 'di', 'Indo', 'Pasifik', '.', 'Indonesia', 'berusaha', 'menjadikan', 'kawasan', 'Indo', 'Pasifik', 'sebagai', 'salah', 'satu', 'sumber', 'utama', 'pertumbuhan', 'ekonomi', ',', 'pusat', 'perdagangan', ',', 'dan', 'industri', 'dunia', '.', 'Turut', 'hadir', 'mendampingi', 'Presiden', 'dalam', 'pelepasan', 'ekspor', 'ini', 'adalah', 'Menteri', 'Perhubungan', 'Budi', 'Karya', 'Sumadi', ',', 'Menteri', 'Perindustrian', 'Airlangga', 'Hartarto', ',', 'Menteri', 'Perdagangan', 'Enggartiasto', 'Lukita', ',', 'Menteri', 'BUMN', 'Rini', 'Soemarno', 'dan', 'Direktur', 'Utama', 'IPC', 'Elvyn', 'G', '.', 'Masassya', '.', 'Polri', 'Awasi', 'Peredaran', 'Bahan', 'Bom', \"'\", 'The', 'Mother', 'of', 'Satan', \"'\"], 'ner_tag': ['B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'B-GPE', 'I-GPE', 'O', 'B-DAT', 'O', 'I-DAT', 'O', 'I-DAT', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRC', 'I-PRC', 'B-PRD', 'O', 'B-PRC', 'I-PRC', 'B-PRD', 'O', 'B-PRC', 'I-PRC', 'O', 'B-PRD', 'O', 'B-PRD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRC', 'I-PRC', 'O', 'O', 'O', 'O', 'B-PRC', 'I-PRC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'B-DAT', 'O', 'I-DAT', 'O', 'I-DAT', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'B-PRD', 'I-PRD', 'I-PRD', 'I-PRD', 'I-PRD', 'O', 'O', 'O', 'O', 'B-PRD', 'O', 'O', 'B-QTY', 'I-QTY', 'I-QTY', 'O', 'O', 'B-QTY', 'I-QTY', 'O', 'B-QTY', 'I-QTY', 'O', 'O', 'B-PRD', 'O', 'O', 'O', 'B-PRD', 'O', 'I-PRD', 'I-PRD', 'O', 'B-PRD', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'B-LOC', 'I-LOC', 'O', 'B-GPE', 'O', 'B-GPE', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORD', 'O', 'O', 'B-PRD', 'O', 'O', 'O', 'O', 'B-MON', 'I-MON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-GPE', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-NOR', 'I-NOR', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-NOR', 'I-NOR', 'B-PER', 'I-PER', 'O', 'B-NOR', 'I-NOR', 'B-PER', 'I-PER', 'O', 'B-NOR', 'I-NOR', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-NOR', 'O', 'O', 'B-PRD', 'I-PRD', 'O', 'B-PRD', 'I-PRD', 'I-PRD', 'I-PRD', 'O']}\n"]}],"source":["print(nergrit[\"train\"][0])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:44.360979Z","iopub.status.busy":"2023-10-23T20:53:44.360732Z","iopub.status.idle":"2023-10-23T20:53:44.858310Z","shell.execute_reply":"2023-10-23T20:53:44.857222Z","shell.execute_reply.started":"2023-10-23T20:53:44.360956Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['I-PRC', 'B-NOR', 'I-QTY', 'I-PER', 'I-CRD', 'B-DAT', 'B-FAC', 'B-REG', 'B-ORD', 'B-LAN', 'B-EVT', 'I-FAC', 'B-LOC', 'B-PRD', 'B-WOA', 'B-ORG', 'I-TIM', 'I-LAN', 'I-REG', 'B-TIM', 'I-EVT', 'I-LOC', 'I-GPE', 'B-QTY', 'B-MON', 'I-DAT', 'B-PRC', 'I-MON', 'B-PER', 'B-CRD', 'B-GPE', 'I-ORG', 'B-LAW', 'I-NOR', 'I-PRD', 'O', 'I-LAW', 'I-ORD', 'I-WOA']\n"]}],"source":["# Extract the 'ner_tag' column from the training set\n","ner_tags_list = nergrit[\"train\"][\"ner_tag\"]\n","\n","# Flatten the list of lists\n","flattened_ner_tags = [tag for sublist in ner_tags_list for tag in sublist]\n","\n","# Get the unique labels\n","unique_labels = list(set(flattened_ner_tags))\n","\n","print(unique_labels)"]},{"cell_type":"markdown","metadata":{},"source":["Based on the documentation in https://huggingface.co/datasets/NusaCrowd/nergrit.\n","<br>\n","Label:\n","\n","'CRD': Cardinal\n","\n","'DAT': Date\n","\n","'EVT': Event\n","\n","'FAC': Facility\n","\n","'GPE': Geopolitical Entity\n","\n","'LAW': Law Entity (such as Undang-Undang)\n","\n","'LOC': Location\n","\n","'MON': Money\n","\n","'NOR': Political Organization\n","\n","'ORD': Ordinal\n","\n","'ORG': Organization\n","\n","'PER': Person\n","\n","'PRC': Percent\n","\n","'PRD': Product\n","\n","'QTY': Quantity\n","\n","'REG': Religion\n","\n","'TIM': Time\n","\n","'WOA': Work of Art\n","\n","'LAN': Language"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:44.859812Z","iopub.status.busy":"2023-10-23T20:53:44.859527Z","iopub.status.idle":"2023-10-23T20:53:44.867602Z","shell.execute_reply":"2023-10-23T20:53:44.866748Z","shell.execute_reply.started":"2023-10-23T20:53:44.859787Z"},"trusted":true},"outputs":[],"source":["labels_dict = {\n","    'O': 0,\n","    'B-CRD': 1, 'I-CRD': 2,\n","    'B-DAT': 3, 'I-DAT': 4,\n","    'B-EVT': 5, 'I-EVT': 6,\n","    'B-FAC': 7, 'I-FAC': 8,\n","    'B-GPE': 9, 'I-GPE': 10,\n","    'B-LAW': 11, 'I-LAW': 12,\n","    'B-LOC': 13, 'I-LOC': 14,\n","    'B-MON': 15, 'I-MON': 16,\n","    'B-NOR': 17, 'I-NOR': 18,\n","    'B-ORD': 19, 'I-ORD': 20,\n","    'B-ORG': 21, 'I-ORG': 22,\n","    'B-PER': 23, 'I-PER': 24,\n","    'B-PRC': 25, 'I-PRC': 26,\n","    'B-PRD': 27, 'I-PRD': 28,\n","    'B-QTY': 29, 'I-QTY': 30,\n","    'B-REG': 31, 'I-REG': 32,\n","    'B-TIM': 33, 'I-TIM': 34,\n","    'B-WOA': 35, 'I-WOA': 36,\n","    'B-LAN': 37, 'I-LAN': 38\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:44.868951Z","iopub.status.busy":"2023-10-23T20:53:44.868675Z","iopub.status.idle":"2023-10-23T20:53:46.242335Z","shell.execute_reply":"2023-10-23T20:53:46.241584Z","shell.execute_reply.started":"2023-10-23T20:53:44.868917Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7b0eb760d224ff78ac4dfe477cef967","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5efa77a57e17457fa7ffcd3f124b1baa","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce7639696cc04e598fe6913ea9dd2ab2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"230af9b50dd4466fa628179087e24ed2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10056b939a354a01b4de8306efcc3069","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# use IndoBERT\n","tokenizer = AutoTokenizer.from_pretrained('indolem/indobert-base-uncased')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:46.243686Z","iopub.status.busy":"2023-10-23T20:53:46.243384Z","iopub.status.idle":"2023-10-23T20:53:46.261368Z","shell.execute_reply":"2023-10-23T20:53:46.260563Z","shell.execute_reply.started":"2023-10-23T20:53:46.243661Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['[CLS]',\n"," 'indonesia',\n"," 'mengekspor',\n"," 'produk',\n"," 'industri',\n"," 'skala',\n"," 'besar',\n"," 'ke',\n"," 'amerika',\n"," 'serikat']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["example = nergrit[\"train\"][0]\n","tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","tokens[:10]"]},{"cell_type":"markdown","metadata":{},"source":["Based on the documentation:\n","\n","> This adds some special tokens [CLS] and [SEP] and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You'll need to realign the tokens and labels by:\n","\n","1. Mapping all tokens to their corresponding word with the word_ids method.\n","2. Assigning the label -100 to the special tokens [CLS] and [SEP] so they're ignored by the PyTorch loss function.\n","3. Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n","\n","So it is needed to realign the token and labels, and truncate the sequence if it is longer than the models maximum length"]},{"cell_type":"markdown","metadata":{},"source":["Based on the reference:\n","\n","> It's more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n","\n","So, creating the data collator."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:46.262979Z","iopub.status.busy":"2023-10-23T20:53:46.262695Z","iopub.status.idle":"2023-10-23T20:53:46.274261Z","shell.execute_reply":"2023-10-23T20:53:46.273461Z","shell.execute_reply.started":"2023-10-23T20:53:46.262955Z"},"trusted":true},"outputs":[],"source":["# Rename the 'labels' column to 'ner_tag'\n","nergrit[\"train\"] = nergrit[\"train\"].rename_column(\"ner_tag\", \"labels\")\n","nergrit[\"validation\"] = nergrit[\"validation\"].rename_column(\"ner_tag\", \"labels\")\n","nergrit[\"test\"] = nergrit[\"test\"].rename_column(\"ner_tag\", \"labels\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:46.279374Z","iopub.status.busy":"2023-10-23T20:53:46.278848Z","iopub.status.idle":"2023-10-23T20:53:46.286813Z","shell.execute_reply":"2023-10-23T20:53:46.285973Z","shell.execute_reply.started":"2023-10-23T20:53:46.279349Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(dataset):\n","    tokenized_inputs = tokenizer(dataset[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, labels_per_example in enumerate(dataset[\"labels\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                token_labels = [labels_dict[label] for label in labels_per_example]\n","                label_ids.append(token_labels[word_idx])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:46.288003Z","iopub.status.busy":"2023-10-23T20:53:46.287749Z","iopub.status.idle":"2023-10-23T20:53:51.465358Z","shell.execute_reply":"2023-10-23T20:53:51.464598Z","shell.execute_reply.started":"2023-10-23T20:53:46.287982Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27a4cb3df83e4ad885e21c2419641f0a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/12551 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fc8434be6184cdca8133782fa7466ec","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2402 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f62e08f406f4fc3ab36cc1094675109","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2526 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_nergrit = nergrit.map(tokenize_and_align_labels, batched=True) #processing in batch"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:51.466786Z","iopub.status.busy":"2023-10-23T20:53:51.466497Z","iopub.status.idle":"2023-10-23T20:53:51.471111Z","shell.execute_reply":"2023-10-23T20:53:51.470194Z","shell.execute_reply.started":"2023-10-23T20:53:51.466761Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["I will be using just accuracy"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:51.472420Z","iopub.status.busy":"2023-10-23T20:53:51.472159Z","iopub.status.idle":"2023-10-23T20:53:52.233240Z","shell.execute_reply":"2023-10-23T20:53:52.232487Z","shell.execute_reply.started":"2023-10-23T20:53:51.472398Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6bac20baea843dc94fc61b10e267591","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:52.234829Z","iopub.status.busy":"2023-10-23T20:53:52.234468Z","iopub.status.idle":"2023-10-23T20:53:52.241767Z","shell.execute_reply":"2023-10-23T20:53:52.240810Z","shell.execute_reply.started":"2023-10-23T20:53:52.234794Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [unique_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [unique_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-Tuning the Model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:52.243059Z","iopub.status.busy":"2023-10-23T20:53:52.242808Z","iopub.status.idle":"2023-10-23T20:53:52.258706Z","shell.execute_reply":"2023-10-23T20:53:52.257867Z","shell.execute_reply.started":"2023-10-23T20:53:52.243037Z"},"trusted":true},"outputs":[],"source":["labels = list(labels_dict.keys())\n","\n","id2label = {i: label for i, label in enumerate(labels)}\n","label2id = {label: i for i, label in id2label.items()}\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:52.259977Z","iopub.status.busy":"2023-10-23T20:53:52.259705Z","iopub.status.idle":"2023-10-23T20:53:52.271862Z","shell.execute_reply":"2023-10-23T20:53:52.271009Z","shell.execute_reply.started":"2023-10-23T20:53:52.259954Z"},"trusted":true},"outputs":[{"data":{"text/plain":["39"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(labels)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:52.273220Z","iopub.status.busy":"2023-10-23T20:53:52.272960Z","iopub.status.idle":"2023-10-23T20:53:52.288029Z","shell.execute_reply":"2023-10-23T20:53:52.287184Z","shell.execute_reply.started":"2023-10-23T20:53:52.273197Z"},"trusted":true},"outputs":[],"source":["batch_size = 16\n","num_train_epochs = 3\n","num_train_steps = (len(tokenized_nergrit[\"train\"]) // batch_size) * num_train_epochs\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n","    num_warmup_steps=0,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:53:52.289894Z","iopub.status.busy":"2023-10-23T20:53:52.289297Z","iopub.status.idle":"2023-10-23T20:54:03.271817Z","shell.execute_reply":"2023-10-23T20:54:03.271091Z","shell.execute_reply.started":"2023-10-23T20:53:52.289862Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8eafb6029fe9413b88bc62f3aeb7c6df","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertForTokenClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["class LoRALayer(tf.keras.layers.Layer):\n","    def __init__(self, original_dim, rank, **kwargs):\n","        super(LoRALayer, self).__init__(**kwargs)\n","        self.original_dim = original_dim\n","        self.rank = rank\n","\n","    def build(self, input_shape):\n","        self.u = self.add_weight(name='u', shape=(self.original_dim, self.rank), initializer='uniform')\n","        self.v = self.add_weight(name='v', shape=(self.rank, self.original_dim), initializer='uniform')\n","        super(LoRALayer, self).build(input_shape)\n","\n","    def call(self, x):\n","        low_rank_matrix = tf.matmul(tf.matmul(self.u, self.v), x)\n","        return x + low_rank_matrix\n","    \n","class LoRAModel(TFAutoModelForTokenClassification):\n","    def __init__(self, *args, **kwargs):\n","        super(LoRAModel, self).__init__(*args, **kwargs)\n","        self.lora = LoRALayer(original_dim=self.config.hidden_size, rank=32)  # You can adjust the rank\n","\n","    def call(self, inputs):\n","        # Apply LoRA to embeddings\n","        embeddings = super(LoRAModel, self).bert.embeddings(inputs)\n","        embeddings = self.lora(embeddings)\n","        \n","        # Continue with the rest of the model's forward pass\n","        outputs = super(LoRAModel, self).bert(inputs, inputs_embeds=embeddings)\n","        return outputs\n","\n","model = LoRAModel.from_pretrained(\n","    \"indolem/indobert-base-uncased\", num_labels=39, id2label=id2label, label2id=label2id, from_pt=True\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:54:03.273131Z","iopub.status.busy":"2023-10-23T20:54:03.272874Z","iopub.status.idle":"2023-10-23T20:54:03.279852Z","shell.execute_reply":"2023-10-23T20:54:03.279001Z","shell.execute_reply.started":"2023-10-23T20:54:03.273108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'index': '0', 'tokens': ['Indonesia', 'mengekspor', 'produk', 'industri', 'skala', 'besar', 'ke', 'Amerika', 'Serikat', '.', 'Ekspor', 'dilakukan', 'melalui', 'Pelabuhan', 'Tanjung', 'Priok', ',', 'Jakarta', 'Utara', ',', 'Selasa', '(', '15', '/', '8', '/', '2018', ')', '.', 'Komoditas', 'yang', 'dikirim', 'terdiri', 'dari', '50', 'persen', 'sepatu', ',', '15', 'persen', 'garmen', ',', '10', 'persen', 'produk', 'karet', ',', 'ban', 'dan', 'turunannya', ',', 'alat', '-', 'alat', 'elektronik', '10', 'persen', ',', 'dan', 'produk', 'lainnya', '15', 'persen', '.', '\"', 'Bukan', 'bahan', 'mentah', ',', 'tetapi', 'sudah', 'bahan', '-', 'bahan', 'produksi', ',', 'produk', '-', 'produk', 'industri', 'yang', 'kita', 'harapkan', 'ini', 'akan', 'meningkatkan', 'ekspor', 'kita', ',', '\"', 'kata', 'Presiden', 'dalam', 'sambutannya', 'pada', 'acara', 'pelepasan', 'ekspor', 'di', 'Jakarta', 'International', 'Container', 'Terminal', '(', 'JICT', ')', ',', 'Selasa', '(', '15', '/', '5', '/', '2018', ')', '.', 'Ekspor', 'tersebut', 'menggunakan', 'Kapal', 'MV', 'CMA', 'CGM', 'Tage', '.', 'Kapal', 'ini', 'merupakan', 'kapal', 'besar', 'berkapasitas', '10', 'ribu', 'TEUs', 'dan', 'berbobot', '95.263', 'GT', '(', 'Gross', 'Tonnage', ')', '.', 'Kapal', 'ini', 'memiliki', 'layanan', 'Java', '-', 'America', 'Express', '(', 'JAX', ')', 'yang', 'rutin', 'melayani', 'rute', 'Pelabuhan', 'Tanjung', 'Priok', 'ke', 'West', 'Coast', '(', 'LA', 'dan', 'Oakland', ')', 'Amerika', 'Serikat', '(', 'direct', 'call', ')', '.', '\"', 'Artinya', 'pengiriman', 'ini', 'besar', 'sekali', 'dan', 'dilakukan', 'dengan', 'sangat', 'efisien', 'dengan', 'direct', 'call', '.', 'Ini', 'akan', 'menurunkan', 'biaya', 'logistik', 'yang', 'sangat', 'besar', '.', 'Tadi', 'sudah', 'disampaikan', 'oleh', 'Dirut', 'Pelindo', 'bahwa', 'setiap', 'kontainer', 'menghemat', 'biaya', 'kurang', 'lebih', 'USD', '300', 'dan', 'ini', 'akan', 'memberikan', 'daya', 'saing', 'produk', '-', 'produk', 'kita', 'terhadap', 'produk', '-', 'produk', 'dari', 'negara', 'lain', ',', '\"', 'lanjutnya', '.', 'Presiden', 'juga', 'menyampaikan', 'bahwa', 'ekspor', 'ke', 'Amerika', 'Serikat', 'ini', 'menandakan', 'bahwa', 'Indonesia', 'memiliki', 'peran', 'yang', 'sangat', 'strategis', 'dalam', 'geo', 'ekonomi', 'di', 'Indo', 'Pasifik', '.', 'Indonesia', 'berusaha', 'menjadikan', 'kawasan', 'Indo', 'Pasifik', 'sebagai', 'salah', 'satu', 'sumber', 'utama', 'pertumbuhan', 'ekonomi', ',', 'pusat', 'perdagangan', ',', 'dan', 'industri', 'dunia', '.', 'Turut', 'hadir', 'mendampingi', 'Presiden', 'dalam', 'pelepasan', 'ekspor', 'ini', 'adalah', 'Menteri', 'Perhubungan', 'Budi', 'Karya', 'Sumadi', ',', 'Menteri', 'Perindustrian', 'Airlangga', 'Hartarto', ',', 'Menteri', 'Perdagangan', 'Enggartiasto', 'Lukita', ',', 'Menteri', 'BUMN', 'Rini', 'Soemarno', 'dan', 'Direktur', 'Utama', 'IPC', 'Elvyn', 'G', '.', 'Masassya', '.', 'Polri', 'Awasi', 'Peredaran', 'Bahan', 'Bom', \"'\", 'The', 'Mother', 'of', 'Satan', \"'\"], 'labels': [-100, 9, 0, 0, 0, 0, 0, 0, 9, 10, 0, 0, 0, 0, 7, 8, 8, 0, 9, 10, 0, 3, 0, 4, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 25, 26, 27, 0, 25, 26, 27, 0, 25, 26, 0, 27, 0, 27, 0, 0, -100, 0, 0, 0, 0, 0, 25, 26, 0, 0, 0, 0, 25, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, -100, -100, 14, 0, 13, -100, -100, 0, 0, 3, 0, 4, 0, 4, 0, 4, 0, 0, 0, 0, 0, 27, 28, 28, -100, 28, -100, 28, -100, 0, 0, 0, 0, 27, 0, 0, 29, 30, 30, -100, 0, 0, 29, -100, -100, -100, 30, 0, 29, 30, -100, -100, 0, 0, 27, 0, 0, 0, 27, 0, 28, 28, 0, 27, -100, 0, 0, 0, 0, 0, 7, 8, 8, 0, 13, 14, 0, 9, 0, 9, -100, 0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, -100, 0, 0, 27, 0, 0, 0, 0, 15, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 9, 10, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 13, 14, 0, 9, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 17, 18, 23, 24, 24, -100, 0, 17, 18, 23, 24, -100, -100, 0, 17, 18, 23, -100, -100, -100, 24, -100, 0, 17, 18, 23, 24, -100, 0, 0, 0, 21, -100, 23, -100, -100, 24, 24, 24, -100, -100, 0, 17, 0, -100, 0, 27, 28, 0, 27, 28, 28, 28, -100, 0, -100], 'input_ids': [3, 1718, 25557, 2392, 3123, 6661, 1819, 1500, 2255, 2644, 18, 6922, 2067, 2180, 4728, 4988, 18306, 16, 1892, 2396, 16, 3764, 12, 2334, 19, 28, 19, 6062, 13, 18, 10828, 1497, 5430, 2877, 1542, 3478, 3056, 9024, 16, 2334, 3056, 29920, 16, 2100, 3056, 2392, 9648, 16, 5464, 1501, 12563, 1519, 16, 3161, 17, 3161, 6602, 2100, 3056, 16, 1501, 2392, 2141, 2334, 3056, 18, 6, 2054, 2672, 8508, 16, 1925, 1798, 2672, 17, 2672, 3506, 16, 2392, 17, 2392, 3123, 1497, 1732, 16606, 1540, 1634, 3552, 6922, 1732, 16, 6, 1951, 2134, 1558, 22101, 1560, 2749, 15727, 6922, 1485, 1892, 5582, 2956, 22531, 934, 6796, 12, 2712, 949, 930, 13, 16, 3764, 12, 2334, 19, 25, 19, 6062, 13, 18, 6922, 1676, 2216, 2793, 24555, 5911, 936, 30830, 931, 5944, 929, 18, 2793, 1540, 1709, 2793, 1819, 16740, 2100, 2856, 2042, 1516, 1501, 24594, 9076, 18, 3428, 964, 20382, 12, 31077, 4608, 2056, 3599, 13, 18, 2793, 1540, 1842, 4616, 14000, 17, 13208, 17028, 12, 7276, 962, 13, 1497, 7478, 5906, 7692, 4728, 4988, 18306, 1500, 6929, 25776, 12, 2155, 1501, 26937, 3284, 13, 2255, 2644, 12, 21111, 14858, 13, 18, 6, 4136, 8841, 1540, 1819, 2268, 1501, 2067, 1545, 1906, 10094, 1545, 21111, 14858, 18, 1540, 1634, 5824, 3592, 9963, 1497, 1906, 1819, 18, 3906, 1798, 4680, 1617, 18900, 1748, 6292, 1737, 2189, 21000, 16768, 3592, 2677, 1716, 15168, 5178, 1501, 1540, 1634, 2308, 3296, 16112, 2392, 17, 2392, 1732, 1973, 2392, 17, 2392, 1542, 1806, 1690, 16, 6, 16961, 18, 2134, 1614, 4905, 1737, 6922, 1500, 2255, 2644, 1540, 12241, 1737, 1718, 1842, 3631, 1497, 1906, 6783, 1558, 3163, 928, 2498, 1485, 9973, 8420, 18, 1718, 3519, 4429, 2627, 9973, 8420, 1624, 1911, 1713, 2530, 2408, 4690, 2498, 16, 2424, 3962, 16, 1501, 3123, 1950, 18, 4185, 4112, 11623, 2134, 1558, 15727, 6922, 1540, 1581, 2556, 8961, 5611, 2423, 25012, 2046, 16, 2556, 12883, 18000, 5255, 10095, 928, 16, 2556, 3962, 3261, 19167, 13209, 2088, 4676, 1636, 16, 2556, 8599, 16138, 29688, 2918, 1501, 4260, 2408, 4918, 949, 2393, 18426, 935, 49, 18, 2106, 13292, 1494, 18, 4095, 23793, 939, 11406, 2672, 4064, 11, 1969, 22584, 2219, 2688, 1476, 11, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["print(tokenized_nergrit[\"train\"][0])"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:54:03.281697Z","iopub.status.busy":"2023-10-23T20:54:03.281120Z","iopub.status.idle":"2023-10-23T20:54:03.871786Z","shell.execute_reply":"2023-10-23T20:54:03.871001Z","shell.execute_reply.started":"2023-10-23T20:54:03.281660Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["tf_train_set = model.prepare_tf_dataset(\n","    tokenized_nergrit[\"train\"],\n","    shuffle=True,\n","    batch_size=16,\n","    collate_fn=data_collator,\n",")\n","\n","tf_validation_set = model.prepare_tf_dataset(\n","    tokenized_nergrit[\"validation\"],\n","    shuffle=False,\n","    batch_size=16,\n","    collate_fn=data_collator,\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:54:03.873324Z","iopub.status.busy":"2023-10-23T20:54:03.872977Z","iopub.status.idle":"2023-10-23T20:54:03.893643Z","shell.execute_reply":"2023-10-23T20:54:03.892841Z","shell.execute_reply.started":"2023-10-23T20:54:03.873292Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer=optimizer)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:54:03.895045Z","iopub.status.busy":"2023-10-23T20:54:03.894772Z","iopub.status.idle":"2023-10-23T20:54:06.224527Z","shell.execute_reply":"2023-10-23T20:54:06.223322Z","shell.execute_reply.started":"2023-10-23T20:54:03.895022Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning https://huggingface.co/apwic/indobert-base-uncased-lora-nergrit into local empty directory.\n"]}],"source":["# create the callback for the model\n","metric_callback = KerasMetricCallback(\n","    metric_fn=compute_metrics, \n","    eval_dataset=tf_validation_set,\n",")\n","\n","push_to_hub_callback = PushToHubCallback(\n","    output_dir=\"indobert-base-uncased-lora-nergrit\",\n","    tokenizer=tokenizer\n",")\n","\n","callbacks = [metric_callback, push_to_hub_callback]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T20:54:06.226242Z","iopub.status.busy":"2023-10-23T20:54:06.225847Z","iopub.status.idle":"2023-10-23T21:22:51.491294Z","shell.execute_reply":"2023-10-23T21:22:51.490351Z","shell.execute_reply.started":"2023-10-23T20:54:06.226206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","784/784 [==============================] - ETA: 0s - loss: 0.4709"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["784/784 [==============================] - 198s 224ms/step - loss: 0.4709 - val_loss: 0.2069 - accuracy: 0.9399\n","Epoch 2/10\n","784/784 [==============================] - 177s 226ms/step - loss: 0.1823 - val_loss: 0.1855 - accuracy: 0.9441\n","Epoch 3/10\n","784/784 [==============================] - 178s 227ms/step - loss: 0.1402 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 4/10\n","784/784 [==============================] - 164s 209ms/step - loss: 0.1287 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 5/10\n","784/784 [==============================] - 163s 208ms/step - loss: 0.1280 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 6/10\n","784/784 [==============================] - 163s 207ms/step - loss: 0.1278 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 7/10\n","784/784 [==============================] - 164s 209ms/step - loss: 0.1285 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 8/10\n","784/784 [==============================] - 164s 209ms/step - loss: 0.1279 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 9/10\n","784/784 [==============================] - 165s 210ms/step - loss: 0.1289 - val_loss: 0.1806 - accuracy: 0.9468\n","Epoch 10/10\n","784/784 [==============================] - 163s 208ms/step - loss: 0.1277 - val_loss: 0.1806 - accuracy: 0.9468\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f1706dfa770>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=10, callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
