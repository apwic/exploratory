{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDaXcllIrCxl"
   },
   "source": [
    "# Sequential Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt to Fine-Tune IndoBERT Model for Sequential Labelling. Check this [Huggingface](https://huggingface.co/apwic/indobert-base-uncased-finetuned-nergrit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKRD3I4rrCxn"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oTsWcokirCxn"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, create_optimizer, TFAutoModelForTokenClassification, pipeline\n",
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPnJa0xbrCxp"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "874f3ce333de420b8d1530c87793b804",
      "0f203608766d47d7b0a99e265afe2465",
      "b71734abaf844c08b4ae6659fa2beb43",
      "1140446e4c7944bd80976a37a2e6349a",
      "54be54864f2144ec94cd7ea495f5f153",
      "2604bcd4c5d04e1aacc75c897dd9e19e",
      "2d17a14cfda24de4b6e470ff58e08fec",
      "d5a07b9cb933406f800c9bd2e14f423d",
      "e6bcdcb745274569bbf915ca366411f1",
      "60169e60dc4348f4ae4eda55c751bee6",
      "d82ca1565bee4e668e34f72f11545734",
      "e26d2a55ef804b44b927bf317895e068",
      "fb06ba6784c44ed6b1a0e1c977dda642",
      "bbe9ffaa50e44662a76fc57daa8fd51e",
      "2b6d1dd5d6ba46e1a1f4c9c196458547",
      "8b7943b5f2f54a2892d5fb6e71cd33ac",
      "fa1b73455c9f47ff91188801e37d2f91",
      "4e2c4ba7d15448b4a4c2118eb3eadca4",
      "2bd6fc91f99b4422a7e72354db04aa75",
      "fbec4238fb864bcea3b83da25d8d0f16",
      "7b7625866d6d445e81082a49ec31868d",
      "9ccd7d0083b34dd19c173cc283520216",
      "86e8868aa5c34b238604518c06367e00",
      "ff12a793c47d4d37a30c4606ade85dd8",
      "e5035d2a806b4f91a5f92328e527d038",
      "344110ac2ff14ca2a651a5b19426b42c",
      "801caff17dc244a199ec00a93a30089a",
      "569f4d64242642a9a7c77abee8c636f1",
      "ba08c5406427466b970fa8a774c69c2c",
      "ab912e90e6b54b6a87ef9d10fae56f51",
      "dda0af0f78d44c7685873cc7d79f1fb8",
      "b441e7d064d34e2899109523bbf4f219",
      "fabb5bcec7f443789f9080714431a7aa",
      "793c6b0363274cffbfa72f6783453e38",
      "08374efa41584e76ac33a6b42072e922",
      "17cad0c0c18d41f08c7620239ecaf35a",
      "01ceb7b4284b4be68f1e65915e9b8a84",
      "d12bfe5648334ceb87598dc09599a794",
      "3135774550824863828cd17268888afe",
      "1a6dbb36235444fb845232a6ae44b1c6",
      "095754bc71564ca9b9ee167f948ffe59",
      "8fe23695726f47b4b243ed192e3af8fd",
      "dff763727a6d4f1ca6a2778d06047d10",
      "d48e376546b743d6a71dc40dd6ca2728",
      "65ac910141d046c3a02c466f30a40317",
      "e5496feac7b74925862bf04a9c545281",
      "1e9c209639b643c4b9ffc4746ad7b80f",
      "68a9b928621649f0adafe37432324b55",
      "1d6ede094b014eed85766fdc199420b4",
      "fd9590dabfd44e6b9e8d02b721f34e2b",
      "9f27eef5735b404c9faa3f7a7e89f04e",
      "bcef08fae55d4671ac5588219599afa7",
      "84594b277a4e41409b733a94bf830252",
      "c065b6a01ed148c885521b7f6ba2d66f",
      "c9529ea13b264ca185fd78a079ea9513",
      "522a2ef1009e4d3c92a6939953d17cc3",
      "c4de2615d1204a8a9fd4803ee021d41f",
      "747b995985b248a6b557023d660a54c2",
      "f617c0b37fc64242bf19926439abb956",
      "20300d1656d4427d956aa8dc08b99670",
      "3dfea592e37c41a0966d3ecc7c9f29c3",
      "e0266b73bcd64c4481b8c0a7762c565e",
      "4da8c9b3898b46b1b91ee9892bbc6b24",
      "9e51076a713b4c2586e9aa1b4a05019b",
      "194e80c91fc9492e9d956805be3b3da4",
      "e9a27d5f0ed34101a028bd2fb11dac0c",
      "dccb5205e0c1418f88c8424bd7e918d9",
      "1ad1d7a60d2e4550b1b1e19e82cdcca7",
      "885c091f90a146408c7d9988d36fbc41",
      "78fa082cc7a042449855e4a59c3c9ac3",
      "8a6bd1fc90e548eb882a95aa0619181e",
      "4187b4953ad249e688868665c61e99fd",
      "735b1e86191f45e8913f08fa912c69d8",
      "3c6703f197a74b83aa28bd045bccee60",
      "98bdf7405a964304b5163113b6bbf493",
      "ffbe4ca980d24b8daaa9e80ed661d6b0",
      "95b96d3dca2a410c8c1a93e16b9f9fed"
     ]
    },
    "id": "WrnPCgYLrCxp",
    "outputId": "40d8df05-3a49-432a-ca2f-8157ab364af6"
   },
   "outputs": [],
   "source": [
    "nergrit = load_dataset('id_nergrit_corpus', 'ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OCW_Bm2rCxq",
    "outputId": "e69f1a9c-726e-4854-d366-346c86228acd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 12532\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2521\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nergrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkuazA1orCxr",
    "outputId": "436cfbf5-3dd2-482c-d55c-c2fb2e80f498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CRD',\n",
       " 'B-DAT',\n",
       " 'B-EVT',\n",
       " 'B-FAC',\n",
       " 'B-GPE',\n",
       " 'B-LAN',\n",
       " 'B-LAW',\n",
       " 'B-LOC',\n",
       " 'B-MON',\n",
       " 'B-NOR',\n",
       " 'B-ORD',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'B-PRC',\n",
       " 'B-PRD',\n",
       " 'B-QTY',\n",
       " 'B-REG',\n",
       " 'B-TIM',\n",
       " 'B-WOA',\n",
       " 'I-CRD',\n",
       " 'I-DAT',\n",
       " 'I-EVT',\n",
       " 'I-FAC',\n",
       " 'I-GPE',\n",
       " 'I-LAN',\n",
       " 'I-LAW',\n",
       " 'I-LOC',\n",
       " 'I-MON',\n",
       " 'I-NOR',\n",
       " 'I-ORD',\n",
       " 'I-ORG',\n",
       " 'I-PER',\n",
       " 'I-PRC',\n",
       " 'I-PRD',\n",
       " 'I-QTY',\n",
       " 'I-REG',\n",
       " 'I-TIM',\n",
       " 'I-WOA',\n",
       " 'O']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = nergrit[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description for this Label\n",
    "\n",
    "The letter that prefixes each ner_tag indicates the token position of the entity:\n",
    "- B- indicates the beginning of an entity.\n",
    "- I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
    "- 0 indicates the token doesn't correspond to any entity.\n",
    "\n",
    "While, each of the tokens description in here:\n",
    "- 'CRD': Cardinal\n",
    "- 'DAT': Date\n",
    "- 'EVT': Event\n",
    "- 'FAC': Facility\n",
    "- 'GPE': Geopolitical Entity\n",
    "- 'LAW': Law Entity (such as Undang-Undang)\n",
    "- 'LOC': Location\n",
    "- 'MON': Money\n",
    "- 'NOR': Political Organization\n",
    "- 'ORD': Ordinal\n",
    "- 'ORG': Organization\n",
    "- 'PER': Person\n",
    "- 'PRC': Percent\n",
    "- 'PRD': Product\n",
    "- 'QTY': Quantity\n",
    "- 'REG': Religion\n",
    "- 'TIM': Time\n",
    "- 'WOA': Work of Art\n",
    "- 'LAN': Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rXV-HitrCxs"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a1646cc7ca9e43f0bde8a6b392301407",
      "fa7e2c6aae13482b9b7e757a9ea22ef4",
      "9ebd5057532f4cf6828de65f83db6895",
      "6d49d5907495425babfb477e1fd53084",
      "7f91b5ca55874b9b8d023f70d773c6f6",
      "e78d3b095f6548b6ac12ea5332c1fcb9",
      "4b717a22d48445dfb41fbe44dc4d010c",
      "3b6df33d424844298a555496eb45be55",
      "c44a3c7d56b84fd7bd50303b8442e119",
      "1057a3468d004953b77aefd36604e588",
      "8293b2bc37e94e3bb623adf6385aeba8",
      "34141d72576e4812a5e589d0644e083a",
      "b39f68e6fd344a85a239581e2ff90eb7",
      "954c9b41511a4f54b7517e27a36150e2",
      "be4af9905b5b4b20a78f3aaa4af4c501",
      "eb6d00e86d2b48d98aaa68c144670697",
      "db7d0f9440f8499fa501969237b31621",
      "2ca8fbff311f4fd99c73377719f60e1d",
      "5b08789e25f548bb99923d7efb9150d5",
      "874902167f944a1dabe40a388f685b62",
      "bf9d75f83e374a22bd636626f22a570b",
      "badacfb1da46436092de980b38c010f7",
      "412b2fb75f3b4b54a7c18770dff9e41a",
      "d10b0343d2424e678f6dc7a7e76afade",
      "b5dd7dc766584707a6d3056075a1c2a4",
      "60607377fe4f44429b841ea15fdf5185",
      "40dc67ff3ee84072bf63ea77d9fe9bad",
      "96c78019368a46a8975dde9c23a5fc9a",
      "abfb3553f69f4a40aee1518cccde1915",
      "fe69537007264c41b4a227ac26e9fd1a",
      "d9f1a7f23a3b452293216d71801a8b42",
      "66a1115de1df459587099233938be8f8",
      "b9783eba0c8d474f8e3476fef17fea4f",
      "3501ab0e954941cb9c42aea571ba6d93",
      "44250b2f709a403e95e99497fb9dec37",
      "0966c2c2b3ad41228c01ceb84db3acfd",
      "298884a684f24ab3b3484391e2a29b61",
      "b30ff814a25c402888f1e697f685b96b",
      "59e7a0e431a34577858e1f5085ea04f2",
      "56aa1d5c22b7460691118a4790b9fa7e",
      "d9ad2b2a9df0478dadc820939848d495",
      "7d080c5953e347339052695ad558f9f2",
      "6038052f2e4b435ab5573e7a5c6e7a49",
      "e4c849c3bdbd4af386866103b397ab9d",
      "d5da4c1ef119469ba8a572f9c653d051",
      "082b85c400d74449ae1bd3b11ad21c81",
      "da4992b63a414d4c8b04b62f2955493a",
      "3cd6895402014a4fb0dc55700b3a5dfa",
      "a275d4b6b4ef4180b410b224fc8094eb",
      "5607bbf988cc4d81823bfeed9496db60",
      "3b99dfcb414f4e34af3ec2797fd52b58",
      "ef118abf521f4c9e9e4e1e975e33753c",
      "60bac1f1c4d04e4596d16d5e9c06f0e1",
      "79088e5dac1d452dacac10766789f660",
      "6e718e66acdd42a4b27049827dd12209"
     ]
    },
    "id": "_agtn8-7rCxs",
    "outputId": "9b6af3d3-e9f1-4599-c5b8-0f3b28e2fe0d"
   },
   "outputs": [],
   "source": [
    "# use IndoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('indolem/indobert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNsc_iMPrCxt",
    "outputId": "a1b15ce0-5320-462d-c697-455b173426ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'indonesia',\n",
       " 'mengekspor',\n",
       " 'produk',\n",
       " 'industri',\n",
       " 'skala',\n",
       " 'besar',\n",
       " 'ke',\n",
       " 'amerika',\n",
       " 'serikat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = nergrit[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mdk1quBrCxt"
   },
   "source": [
    "Based on the documentation:\n",
    "\n",
    "> This adds some special tokens [CLS] and [SEP] and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You'll need to realign the tokens and labels by:\n",
    "\n",
    "1. Mapping all tokens to their corresponding word with the word_ids method.\n",
    "2. Assigning the label -100 to the special tokens [CLS] and [SEP] so they're ignored by the PyTorch loss function.\n",
    "3. Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
    "\n",
    "So it is needed to realign the token and labels, and truncate the sequence if it is longer than the models maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GdFwEezsrCxt"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    tokenized_inputs = tokenizer(dataset[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "718a1c44434242ffa1acd82faad922a4",
      "45ad3b55dbbd401ea23eb17f263f56a2",
      "6d1924ee020e45758022f1d69a2c8196",
      "4eaccccc86054235a3c03e8af55465ca",
      "b6e2a3ae10f749cb8b78126d49570c91",
      "2b1c23ae30bd40668f9bbfbc61968e09",
      "def566a34dcb45379c3b2dfbbec25f54",
      "d4052a17cde54cf697a2682cd4d81259",
      "83a206a6872b4ea4b367fd2eae6a55ff",
      "e84ec525d85f40c9a7cb3b25f94350b4",
      "f5cd980833f44e49bfdb2535da108b41",
      "596542ebea2841908d3507cb478373d5",
      "b60fa0b6194e48ebace0319e6d6609b8",
      "fb7b8b1d90214156a1740910be1a8a0d",
      "93d1706ca0154ea7a0fc9487e0f254eb",
      "c2b2ba72d55b42e480f28e5680277b70",
      "be0efe5fb5f74778be748c72b4088acd",
      "78e2113c559946389e57d5cd5da6b063",
      "0d583d9e54ed4478bf1a8f22cff7fc64",
      "45379fa89dd9478987d5b12e47e5cc9a",
      "85d4fa287dbf40808a7a7ceec7881399",
      "f2faac9894794008a2a28a20e9420e76",
      "5006f56778a24b9ab32dff4c04a3a115",
      "8fad131d9e624c2a835734c55b5ff840",
      "85b51e93c4954f4a940c08843c06d152",
      "64ef847c36b84aa983daf03f8582267b",
      "17e270763bf14eff819a67eb020bedf0",
      "55379c4982bb4f51a55a6e9ca28e9fce",
      "e1d4497aac2f4354abe38e47df377115",
      "7cef9cbafae74b5ca89f16fb12197a37",
      "a8123f1f588440b2b0b00e872364a155",
      "2b3150544b01489bb652a32da38344b3",
      "e413e304d66f440f80d5f7fd12515c57"
     ]
    },
    "id": "5gKslZuarCxu",
    "outputId": "5b16e2b8-6c84-415e-f39f-7480cc59ac14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa937a80cfce49028951190587136728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_nergrit = nergrit.map(tokenize_and_align_labels, batched=True) #processing in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nGl3Xe9rCxu"
   },
   "source": [
    "Based on the reference:\n",
    "\n",
    "> It's more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "\n",
    "So, creating the data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "guwB_K9srCxu"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA00Zb18rCxu"
   },
   "source": [
    "## Metrics Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJclGoY4rCxu"
   },
   "source": [
    "Evaluation for this models will only use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f60a07de4afd41b691494f55ffe01112",
      "2c966b2d77da494085591ffea51aa904",
      "6edf0b8a4d67492394cd1f06b99f042a",
      "d612f763ca5a4e28b3134bb48acbc0a3",
      "212b2bbd7f8942bdbaa8aa834a933efd",
      "6c3cd7e2b0fa4c37aebff7a1961fa708",
      "d1477c20e7114724891414267049c062",
      "ed7e9a7dedfa43f9b73aa1cc02c977ee",
      "da6b13678b8949ce89c28d77346a0353",
      "e660c8ca5856476c8fed80e9c295ab00",
      "9ee45c7a5ccc4f6d9c0b40f61ddc3e5c"
     ]
    },
    "id": "d8DTL6LHrCxu",
    "outputId": "d7a26f5c-b0df-4586-b0fe-299374c22b1c"
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "haE-t42NrCxu"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enrsB1VBrCxv"
   },
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mdel2Jf3rCxv"
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"B-CRD\", \"B-DAT\", \"B-EVT\", \"B-FAC\", \"B-GPE\", \"B-LAN\", \"B-LAW\", \"B-LOC\", \"B-MON\", \"B-NOR\",\n",
    "    \"B-ORD\", \"B-ORG\", \"B-PER\", \"B-PRC\", \"B-PRD\", \"B-QTY\", \"B-REG\", \"B-TIM\", \"B-WOA\",\n",
    "    \"I-CRD\", \"I-DAT\", \"I-EVT\", \"I-FAC\", \"I-GPE\", \"I-LAN\", \"I-LAW\", \"I-LOC\", \"I-MON\", \"I-NOR\",\n",
    "    \"I-ORD\", \"I-ORG\", \"I-PER\", \"I-PRC\", \"I-PRD\", \"I-QTY\", \"I-REG\", \"I-TIM\", \"I-WOA\", \"O\",\n",
    "]\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYSFqcAIrCxv",
    "outputId": "52f2172c-969e-4ce8-e0fa-9844de652d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-54hMkZXrCxv"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_nergrit[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJHlbXdxrCxv",
    "outputId": "bbcdf55c-9009-489d-eda3-4e2f6820adcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    \"indolem/indobert-base-uncased\", \n",
    "    num_labels=39,          # set the num labels to 39\n",
    "    id2label=id2label, \n",
    "    label2id=label2id, \n",
    "    from_pt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OqIVQnDqrCxv"
   },
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_nergrit[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_nergrit[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OMyqbDdsrCxv"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRzSMZqHrCxw",
    "outputId": "319bf789-95aa-427f-ecab-3346f6fb41d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/indobert-base-uncased-finetuned-nergrit is already a clone of https://huggingface.co/apwic/indobert-base-uncased-finetuned-nergrit. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "WARNING:huggingface_hub.repository:/content/indobert-base-uncased-finetuned-nergrit is already a clone of https://huggingface.co/apwic/indobert-base-uncased-finetuned-nergrit. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# create the callback for the model\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"indobert-base-uncased-finetuned-nergrit\",  # change this based on the output repo desired\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8HH4iKPrCxw",
    "outputId": "33509592-65e3-482d-e7b8-129ab5d618a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.3745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1566/1566 [==============================] - 318s 192ms/step - loss: 0.3745 - val_loss: 0.1815 - accuracy: 0.9461\n",
      "Epoch 2/3\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.1501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1566/1566 [==============================] - 317s 202ms/step - loss: 0.1501 - val_loss: 0.1795 - accuracy: 0.9475\n",
      "Epoch 3/3\n",
      "1566/1566 [==============================] - ETA: 0s - loss: 0.1072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1566/1566 [==============================] - 321s 205ms/step - loss: 0.1072 - val_loss: 0.1791 - accuracy: 0.9491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a7feabb2ad0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, \n",
    "          validation_data=tf_validation_set, \n",
    "          epochs=3,                 # small epochs to fasten the training\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK0gMW5YwjTj"
   },
   "source": [
    "## Inferencing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T-lrFGPjrCxw"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Jakarta, Maret 1998\n",
    "Di sebuah senja, di sebuah rumah susun di Jakarta, mahasiswa bernama Biru Laut disergap empat lelaki tak dikenal. Bersama kawan-kawannya, Daniel Tumbuan, Sunu Dyantoro, Alex Perazon, dia dibawa ke sebuah tempat yang tak dikenal. Berbulan-bulan mereka disekap, diinterogasi, dipukul, ditendang, digantung, dan disetrum agar bersedia menjawab satu pertanyaan penting: siapakah yang berdiri di balik gerakan aktivis dan mahasiswa saat itu.\n",
    "Jakarta, Juni 1998\n",
    "Keluarga Arya Wibisono, seperti biasa, pada hari Minggu sore memasak bersama, menyediakan makanan kesukaan Biru Laut. Sang ayah akan meletakkan satu piring untuk dirinya, satu piring untuk sang ibu, satu piring untuk Biru Laut, dan satu piring untuk si bungsu Asmara Jati. Mereka duduk menanti dan menanti. Tapi Biru Laut tak kunjung muncul.\n",
    "Jakarta, 2000\n",
    "Asmara Jati, adik Biru Laut, beserta Tim Komisi Orang Hilang yang dipimpin Aswin Pradana mencoba mencari jejak mereka yang hilang serta merekam dan mempelajari testimoni mereka yang kembali. Anjani, kekasih Laut, para orangtua dan istri aktivis yang hilang menuntut kejelasan tentang anggota keluarga mereka. Sementara Biru Laut, dari dasar laut yang sunyi bercerita kepada kita, kepada dunia tentang apa yang terjadi pada dirinya dan kawan-kawannya.\n",
    "Laut Bercerita, novel terbaru Leila S. Chudori, bertutur tentang kisah keluarga yang kehilangan, sekumpulan sahabat yang merasakan kekosongan di dada, sekelompok orang yang gemar menyiksa dan lancar berkhianat, sejumlah keluarga yang mencari kejelasan akan anaknya, dan tentang cinta yang tak akan luntur.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KW-hypHDwtup",
    "outputId": "d70abc6e-f6df-4564-b089-44ca45ce8907"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at apwic/indobert-base-uncased-finetuned-nergrit were not used when initializing TFBertForTokenClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at apwic/indobert-base-uncased-finetuned-nergrit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# use pipeline to easen inferencing the model\n",
    "classifier = pipeline(\"ner\", model=\"apwic/indobert-base-uncased-finetuned-nergrit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pl94banxDn9",
    "outputId": "cd6d8407-c430-4cb5-c93f-ccb066b7f3c3"
   },
   "outputs": [],
   "source": [
    "classified_text = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-GPE</td>\n",
       "      <td>0.996624</td>\n",
       "      <td>1</td>\n",
       "      <td>jakarta</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-DAT</td>\n",
       "      <td>0.996588</td>\n",
       "      <td>3</td>\n",
       "      <td>maret</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-DAT</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>4</td>\n",
       "      <td>1998</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-GPE</td>\n",
       "      <td>0.995673</td>\n",
       "      <td>14</td>\n",
       "      <td>jakarta</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.903852</td>\n",
       "      <td>18</td>\n",
       "      <td>biru</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.714538</td>\n",
       "      <td>19</td>\n",
       "      <td>laut</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-CRD</td>\n",
       "      <td>0.923469</td>\n",
       "      <td>22</td>\n",
       "      <td>empat</td>\n",
       "      <td>108</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.995122</td>\n",
       "      <td>32</td>\n",
       "      <td>daniel</td>\n",
       "      <td>158</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>33</td>\n",
       "      <td>tum</td>\n",
       "      <td>165</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>34</td>\n",
       "      <td>##buan</td>\n",
       "      <td>168</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity     score  index     word  start  end\n",
       "0  B-GPE  0.996624      1  jakarta      0    7\n",
       "1  B-DAT  0.996588      3    maret      9   14\n",
       "2  I-DAT  0.993151      4     1998     15   19\n",
       "3  B-GPE  0.995673     14  jakarta     62   69\n",
       "4  B-PER  0.903852     18     biru     89   93\n",
       "5  I-PER  0.714538     19     laut     94   98\n",
       "6  B-CRD  0.923469     22    empat    108  113\n",
       "7  B-PER  0.995122     32   daniel    158  164\n",
       "8  I-PER  0.996632     33      tum    165  168\n",
       "9  I-PER  0.996200     34   ##buan    168  172"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classified_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial showing of the classified_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entities_html(text, classified_text):\n",
    "    # Create a color mapping for each entity type\n",
    "    color_map = {\n",
    "        'GPE': 'yellow',\n",
    "        'DAT': 'blue',\n",
    "        'PER': 'green',\n",
    "        'CRD': 'red',\n",
    "        'EVT': '#FFA500',  # orange\n",
    "        'FAC': '#FFC0CB',  # pink\n",
    "        'LAW': '#FFD700',  # gold\n",
    "        'LOC': '#ADFF2F',  # greenyellow\n",
    "        'MON': '#FA8072',  # salmon\n",
    "        'NOR': '#9370DB',  # mediumpurple\n",
    "        'ORD': '#7B68EE',  # mediumslateblue\n",
    "        'ORG': '#6A5ACD',  # slateblue\n",
    "        'PRC': '#FF69B4',  # hotpink\n",
    "        'PRD': '#D2B48C',  # tan\n",
    "        'QTY': '#FF6347',  # tomato\n",
    "        'REG': '#DB7093',  # palevioletred\n",
    "        'TIM': '#EEE8AA',  # palegoldenrod\n",
    "        'WOA': '#F08080',  # lightcoral\n",
    "        'LAN': '#BDB76B'   # darkkhaki\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Sort classified_text by start index\n",
    "    classified_text.sort(key=lambda x: x['start'])\n",
    "\n",
    "    html_output = text\n",
    "    shift = 0\n",
    "    \n",
    "    for entity in classified_text:\n",
    "        word = entity['word']\n",
    "        start = entity['start'] + shift\n",
    "        end = entity['end'] + shift\n",
    "        entity_type = entity['entity'].split('-')[-1]  # Extracting main entity type e.g., 'B-GPE' -> 'GPE'\n",
    "        color = color_map.get(entity_type, 'grey')  # Default to grey if entity type is not in our map\n",
    "        \n",
    "        # Wrap the word in a span with background color\n",
    "        span = f\"<span style='background-color: {color}'>{word}</span>\"\n",
    "        \n",
    "        # Replace the word in the text with the highlighted word\n",
    "        html_output = html_output[:start] + span + html_output[end:]\n",
    "        \n",
    "        # Adjust shift based on the added HTML tags\n",
    "        shift += len(span) - (end - start)\n",
    "    \n",
    "    display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the visualization of Sequential Labelling, evaluating the model is already done as the model ist trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background-color: yellow'>jakarta</span>, <span style='background-color: blue'>maret</span> <span style='background-color: blue'>1998</span>\n",
       "Di sebuah senja, di sebuah rumah susun di <span style='background-color: yellow'>jakarta</span>, mahasiswa bernama <span style='background-color: green'>biru</span> <span style='background-color: green'>laut</span> disergap <span style='background-color: red'>empat</span> lelaki tak dikenal. Bersama kawan-kawannya, <span style='background-color: green'>daniel</span> <span style='background-color: green'>tum</span><span style='background-color: green'>##buan</span>, <span style='background-color: green'>sun</span><span style='background-color: green'>##u</span> <span style='background-color: green'>dy</span><span style='background-color: green'>##antoro</span>, <span style='background-color: green'>alex</span> <span style='background-color: green'>per</span><span style='background-color: green'>##az</span><span style='background-color: green'>##on</span>, dia dibawa ke sebuah tempat yang tak dikenal. Berbulan-bulan mereka disekap, diinterogasi, dipukul, ditendang, digantung, dan disetrum agar bersedia menjawab <span style='background-color: red'>satu</span> pertanyaan penting: siapakah yang berdiri di balik gerakan aktivis dan mahasiswa saat itu.\n",
       "<span style='background-color: yellow'>jakarta</span>, <span style='background-color: blue'>juni</span> <span style='background-color: blue'>1998</span>\n",
       "Keluarga <span style='background-color: green'>arya</span> <span style='background-color: green'>wibis</span><span style='background-color: green'>##ono</span>, seperti biasa, pada hari <span style='background-color: blue'>minggu</span> sore memasak bersama, menyediakan makanan kesukaan <span style='background-color: green'>biru</span> <span style='background-color: green'>laut</span>. Sang ayah akan meletakkan <span style='background-color: red'>satu</span> piring untuk dirinya, <span style='background-color: red'>satu</span> piring untuk sang ibu, <span style='background-color: red'>satu</span> piring untuk <span style='background-color: #D2B48C'>biru</span> <span style='background-color: #D2B48C'>laut</span>, dan <span style='background-color: red'>satu</span> piring untuk si bungsu <span style='background-color: green'>asmara</span> <span style='background-color: green'>jati</span>. Mereka duduk menanti dan menanti. Tapi <span style='background-color: #D2B48C'>biru</span> <span style='background-color: #D2B48C'>laut</span> tak kunjung muncul.\n",
       "<span style='background-color: yellow'>jakarta</span>, <span style='background-color: blue'>2000</span>\n",
       "<span style='background-color: green'>asmara</span> <span style='background-color: green'>jati</span>, adik <span style='background-color: green'>biru</span> <span style='background-color: green'>laut</span>, beserta <span style='background-color: #6A5ACD'>tim</span> <span style='background-color: #6A5ACD'>komisi</span> <span style='background-color: #6A5ACD'>orang</span> <span style='background-color: #6A5ACD'>hilang</span> yang dipimpin <span style='background-color: green'>asw</span><span style='background-color: green'>##in</span> <span style='background-color: green'>prad</span><span style='background-color: green'>##ana</span> mencoba mencari jejak mereka yang hilang serta merekam dan mempelajari testimoni mereka yang kembali. <span style='background-color: green'>anj</span><span style='background-color: green'>##ani</span>, kekasih <span style='background-color: green'>laut</span>, para orangtua dan istri aktivis yang hilang menuntut kejelasan tentang anggota keluarga mereka. Sementara <span style='background-color: #D2B48C'>biru</span> <span style='background-color: #D2B48C'>laut</span>, dari dasar laut yang sunyi bercerita kepada kita, kepada dunia tentang apa yang terjadi pada dirinya dan kawan-kawannya.\n",
       "Laut Bercerita, novel terbaru <span style='background-color: green'>le</span><span style='background-color: green'>##ila</span> <span style='background-color: green'>s</span><span style='background-color: green'>.</span> <span style='background-color: green'>chu</span><span style='background-color: green'>##do</span><span style='background-color: green'>##ri</span>, bertutur tentang kisah keluarga yang kehilangan, sekumpulan sahabat yang merasakan kekosongan di dada, sekelompok orang yang gemar menyiksa dan lancar berkhianat, sejumlah keluarga yang mencari kejelasan akan anaknya, dan tentang cinta yang tak akan luntur."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_entities_html(text, classified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_text = \"\"\"Toru Watanabe terlempar jauh ke waktu hamper 20 tahun silam saai ia masih menjadi mahasiswa yang terjerat dalam hubungan pertemanan yang rumit serta pelik, masa-masa seks bebas, serba-serbi nafsu, serta rasa hampa yang menyelimuti seorang gadis badung, Midori, yang memasuki hidupnya, yang membuat Toru Watanabe harus menentukan untuk memprioritaskan antara masa depan atau masa lalu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_classified_text = classifier(other_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>1</td>\n",
       "      <td>tor</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>2</td>\n",
       "      <td>##u</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.996024</td>\n",
       "      <td>3</td>\n",
       "      <td>watan</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.995842</td>\n",
       "      <td>4</td>\n",
       "      <td>##abe</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-QTY</td>\n",
       "      <td>0.986421</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-QTY</td>\n",
       "      <td>0.976840</td>\n",
       "      <td>12</td>\n",
       "      <td>tahun</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.992664</td>\n",
       "      <td>51</td>\n",
       "      <td>mid</td>\n",
       "      <td>253</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.958504</td>\n",
       "      <td>52</td>\n",
       "      <td>##ori</td>\n",
       "      <td>256</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>60</td>\n",
       "      <td>tor</td>\n",
       "      <td>298</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>61</td>\n",
       "      <td>##u</td>\n",
       "      <td>301</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.996089</td>\n",
       "      <td>62</td>\n",
       "      <td>watan</td>\n",
       "      <td>303</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.996042</td>\n",
       "      <td>63</td>\n",
       "      <td>##abe</td>\n",
       "      <td>308</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity     score  index   word  start  end\n",
       "0   B-PER  0.996455      1    tor      0    3\n",
       "1   I-PER  0.988008      2    ##u      3    4\n",
       "2   I-PER  0.996024      3  watan      5   10\n",
       "3   I-PER  0.995842      4  ##abe     10   13\n",
       "4   B-QTY  0.986421     11     20     45   47\n",
       "5   I-QTY  0.976840     12  tahun     48   53\n",
       "6   B-PER  0.992664     51    mid    253  256\n",
       "7   I-PER  0.958504     52  ##ori    256  259\n",
       "8   B-PER  0.996367     60    tor    298  301\n",
       "9   I-PER  0.989379     61    ##u    301  302\n",
       "10  I-PER  0.996089     62  watan    303  308\n",
       "11  I-PER  0.996042     63  ##abe    308  311"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(other_classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background-color: green'>tor</span><span style='background-color: green'>##u</span> <span style='background-color: green'>watan</span><span style='background-color: green'>##abe</span> terlempar jauh ke waktu hamper <span style='background-color: #FF6347'>20</span> <span style='background-color: #FF6347'>tahun</span> silam saai ia masih menjadi mahasiswa yang terjerat dalam hubungan pertemanan yang rumit serta pelik, masa-masa seks bebas, serba-serbi nafsu, serta rasa hampa yang menyelimuti seorang gadis badung, <span style='background-color: green'>mid</span><span style='background-color: green'>##ori</span>, yang memasuki hidupnya, yang membuat <span style='background-color: green'>tor</span><span style='background-color: green'>##u</span> <span style='background-color: green'>watan</span><span style='background-color: green'>##abe</span> harus menentukan untuk memprioritaskan antara masa depan atau masa lalu"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_entities_html(other_text, other_classified_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
