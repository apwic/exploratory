{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import datasets\n",
    "import evaluate\n",
    "import accelerate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"climatebert/climate_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Based for Text Classification\n",
    "\n",
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = dataset[\"train\"][\"text\"]\n",
    "label_train = dataset[\"train\"][\"label\"]\n",
    "\n",
    "text_test = dataset[\"test\"][\"text\"]\n",
    "label_test = dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.5\n",
    "\n",
    "_, train_set = dataset[\"train\"].train_test_split(test_size=TEST_SIZE).values()\n",
    "_, test_set = dataset[\"test\"].train_test_split(test_size=TEST_SIZE).values()\n",
    "\n",
    "mini_ds = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": train_set,\n",
    "        \"test\" : test_set\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets for RNN-Based Models (to compare with BERT-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max words for the vocabulary\n",
    "MAX_WORDS = 10000\n",
    "tokenizer_train = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# fit dataset to tokenizer\n",
    "tokenizer_train.fit_on_texts(text_train)\n",
    "tokenizer_test.fit_on_texts(text_test)\n",
    "\n",
    "# convert dataset to sequence of integer\n",
    "seq_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer_test.texts_to_sequences(text_test)\n",
    "\n",
    "# pad the sequence to fixed_length, will adjust later\n",
    "MAX_SEQ = 250\n",
    "X_train = pad_sequences(sequences=seq_train, maxlen=MAX_SEQ)\n",
    "X_test = pad_sequences(sequences=seq_test, maxlen=MAX_SEQ)\n",
    "\n",
    "# turn the lables into categorical\n",
    "y_train = to_categorical(label_train, 3)\n",
    "y_test = to_categorical(label_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset for the BERT-Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3681.76 examples/s]\n",
      "Map: 100%|██████████| 160/160 [00:00<00:00, 3218.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer from BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(ds):\n",
    "    return tokenizer(ds[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized_ds = mini_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RNN-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "w2v = Word2Vec(sentences=text_train, vector_size=EMBEDDING_DIM, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_train.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in w2v.wv:\n",
    "            embedding_matrix[i] = w2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn_w2v = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# change the weight to embedding_matrix from Word2Vec\n",
    "rnn_w2v.add(Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=True))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn_w2v.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# Bidirectional LSTM layers with dropout and recurrent dropout\n",
    "rnn_w2v.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "rnn_w2v.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "# Batch normalization layer\n",
    "rnn_w2v.add(BatchNormalization())\n",
    "\n",
    "# Dense layer with ReLU activation\n",
    "rnn_w2v.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Dropout layer\n",
    "rnn_w2v.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn_w2v.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# Learning rate reduction on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Compile the RNN model with custom optimizer (Adam) and loss function (categorical_crossentropy)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "rnn_w2v.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 65s 4s/step - loss: 1.0843 - accuracy: 0.4000 - val_loss: 1.0851 - val_accuracy: 0.5188\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.9667 - accuracy: 0.5380 - val_loss: 1.0837 - val_accuracy: 0.5156\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 60s 4s/step - loss: 0.6924 - accuracy: 0.7340 - val_loss: 1.0731 - val_accuracy: 0.4719\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 60s 4s/step - loss: 0.3291 - accuracy: 0.9030 - val_loss: 1.0489 - val_accuracy: 0.4344\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 63s 4s/step - loss: 0.1825 - accuracy: 0.9500 - val_loss: 1.1220 - val_accuracy: 0.2969\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.1152 - accuracy: 0.9680 - val_loss: 1.0956 - val_accuracy: 0.3656\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0516 - accuracy: 0.9920 - val_loss: 1.1209 - val_accuracy: 0.3781\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0348 - accuracy: 0.9950 - val_loss: 1.2401 - val_accuracy: 0.2969\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0265 - accuracy: 0.9960 - val_loss: 1.2651 - val_accuracy: 0.3313\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0209 - accuracy: 0.9960 - val_loss: 1.2877 - val_accuracy: 0.3406\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0267 - accuracy: 0.9970 - val_loss: 1.3974 - val_accuracy: 0.3250\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0257 - accuracy: 0.9940 - val_loss: 1.4990 - val_accuracy: 0.3406\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0271 - accuracy: 0.9900 - val_loss: 1.6014 - val_accuracy: 0.3313\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0175 - accuracy: 0.9960 - val_loss: 1.7098 - val_accuracy: 0.3531\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0213 - accuracy: 0.9950 - val_loss: 2.0762 - val_accuracy: 0.3000\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0182 - accuracy: 0.9970 - val_loss: 1.7898 - val_accuracy: 0.3719\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 1.9653 - val_accuracy: 0.3781\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0197 - accuracy: 0.9980 - val_loss: 2.2507 - val_accuracy: 0.3438\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 59s 4s/step - loss: 0.0140 - accuracy: 0.9980 - val_loss: 2.4185 - val_accuracy: 0.3406\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 58s 4s/step - loss: 0.0117 - accuracy: 0.9980 - val_loss: 2.5619 - val_accuracy: 0.3063\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_w2v_history = rnn_w2v.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 195ms/step - loss: 2.5619 - accuracy: 0.3063\n",
      "Loss:\t2.5619\n",
      "Accuracy:\t0.3063\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BERT-Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=pred, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_training_args = TrainingArguments(output_dir=\"trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    model= bert_model,\n",
    "    args= bert_training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 28:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708220</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704951</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755466</td>\n",
       "      <td>0.756250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.4463630151496362, metrics={'train_runtime': 1743.4434, 'train_samples_per_second': 0.86, 'train_steps_per_second': 0.108, 'total_flos': 198704641536000.0, 'train_loss': 0.4463630151496362, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_result = bert_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7554655075073242,\n",
       " 'eval_accuracy': 0.75625,\n",
       " 'eval_runtime': 74.139,\n",
       " 'eval_samples_per_second': 2.158,\n",
       " 'eval_steps_per_second': 0.27,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RNN-Based to BERT-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 752ms/step\n",
      "Text: sustainable strategy ‘red lines’ for our sustainable strategy range we incorporate a series of proprietary ‘red lines’ in order to ensure the poorest performing companies from an esg perspective are not eligible for investment\\Groundtruth: 0\n",
      "Predicted: 0\n",
      "\n",
      "Text: verizon’s environmental health and safety management system provides a framework for identifying controlling and reducing the risks associated with the environments in which we operate besides regular management system assessments internal and third party compliance audits and inspections are performed annually at hundreds of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions verizon’s environment health and safety efforts are directed and supported by experienced experts around the world that support our operations and facilities\\Groundtruth: 1\n",
      "Predicted: 0\n",
      "\n",
      "Text: in 2019 the company closed a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an affiliate of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cory cogeneration station to saskpower international and its 50 per cent ownership interest in brighton beach power to ontario power generation\\Groundtruth: 1\n",
      "Predicted: 0\n",
      "\n",
      "Text: in december 2020 the auc approved the electricity distribution and natural gas distribution requests to defer the compulsory distribution rate increases which would normally come into effect on january 1 2021 for both businesses the rate relief was requested to defer significant distribution rate increases which would be passed onto end use customers due to the formulaic approach of rate calculations under the auc pbr mechanism electricity distribution and natural gas distribution cited the current economic situation in alberta including the hardships faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to file an application by march 1 2021 outlining the duration of the rate freeze and collection timelines expected deferral values including carrying costs and anticipated impacts to customers\\Groundtruth: 0\n",
      "Predicted: 1\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions\\Groundtruth: 0\n",
      "Predicted: 0\n",
      "\n",
      "Score: 2/10\n"
     ]
    }
   ],
   "source": [
    "# using Word2Vec embedding\n",
    "predictions = rnn_w2v.predict(X_test[:PREDICT_SIZE])\n",
    "score = 0\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    pred = pred.index(max(pred))\n",
    "\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    groundtruth = groundtruth.index(max(groundtruth))\n",
    "\n",
    "    if (groundtruth == pred):\n",
    "        score += 1\n",
    "\n",
    "    print(f\"Text: {text}\\Groundtruth: {groundtruth}\\nPredicted: {pred}\\n\")\n",
    "\n",
    "print(f\"Score: {score}/{PREDICT_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: BB DTVM, by strategic direction, performs extensive asset screening considering socioenvironmental and corporate governance aspects. By means of its own ESG methodology, which uses a combination of positive and negative screening, it ended the period with R$ 648.85 billion in assets subject to this methodology, representing 55.12% of the total assets under management. The asset manager has been managing and distributing 10 investment funds with socioenvironmental characteristics to the different investor segments, that, in December 2020, totaled R$ 3.15 billion in shareholders’ equity.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: The 2019 Integrated Report is the eighth Atlantia’s annual integrated report, prepared based on the International Framework set out by the International Integrated Reporting Council (www.theiirc.org/international-ir-framework/) and drafted in accordance with the GRI Sustainability Reporting Standards published in 2016 by GRI – Global Reporting Initiative, according to the “in accordance core” option. At the end of the document, “GRI Content Index” indicates the content reported in compliance with the GRI. For some information, as indicated in the index above, explicit reference is made to other company documents[2]. The selection of the questions and indicators examined was based on a structured materiality analysis process, aimed at identifying the significant topics for the Group according to their impact on business, their importance for stakeholders and the likelihood and extent of the risks/opportunities associated with them. In line with 2018, the process adopted to define the 2019 matrix of materiality involved interviews with the top management of Atlantia and its main subsidiaries, as well as the use of the Datamaran platform to examine the external perspective and context, through an automated analysis of the multitude of information available from public sources, including company reports, regulations, voluntary initiatives, news and social media The resulting matrix of materiality was submitted to the Atlantia Control, Risk and Corporate Governance Committee during the session held on 14 April 2020 and to the Board of Directors of Atlantia during the meeting held on 17 April 2020, was was approved by the latter together with this Report.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: Strategy Disclose the actual and potential impacts of climate-related risks and opportunities on the organization’s businesses, strategy, and financial planning where such information is material. a) Describe the climate-related risks and opportunities the organization has identified over the short, medium, and long term.\n",
      "Groundtruth: 0\n",
      "Prediction: 1\n",
      "\n",
      "Text: 2 Increased severity/frequency of extreme weather events. Loss or impairment of key manufacturing sites, inability to procure sufficient raw materials, disruption to transportation of raw materials or finished goods, etc. as result of an extreme weather event could disrupt our operations if the response to such an event is not effectively managed and remedied.\n",
      "Groundtruth: 0\n",
      "Prediction: 0\n",
      "\n",
      "Text: The data required for emissions calculation is currently not available for the following: z Scope 1 and scope 3 as they relate to offices outside of South Africa and where there is a lack of reliable data. Emissions associated with the operation and servicing of ATMs, self-service terminals and point-of-sale devices located away from a branch or office premises, and other remote devices. Any other premises or activities owned or operated by us, but not explicitly referenced in this report, such as Nedbank kiosks in retail stores.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: Our aim in 2021 is to be able to report the weighted average carbon intensity of our corporate bonds portfolio. Furthermore, in order to improve monitoring, management and future reporting in this area, we are working towards an improved picture of the emissions intensity of other significant portions of our investment portfolio, where appropriate data and methodologies exist.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: • Customers’ changing preference for environmental friendly product • Shareholders' inclination to invest in companies with sustainable commitment • Reallocation of capitals to cleaner and greener investments by financial institutions. • Significant investment into technologies and infrastructure to address climate change and to look beyond own operations • Increase in operational costs to meet regulatory requirements for GHG emissions and climate change\n",
      "Groundtruth: 2\n",
      "Prediction: 0\n",
      "\n",
      "Text: Climate Change WHAT THIS MEANS? The Government’s latest climate change risk assessment identifies flood risk, and particularly flooding from heavy downpours, as one of the key climate threats for the UK. This must be viewed alongside stresses on water resources, threats to biodiversity and natural habitats, and the repercussions for the UK from climate change impacts abroad.\n",
      "Groundtruth: 0\n",
      "Prediction: 0\n",
      "\n",
      "Text: In 2019, we created a role dedicated to enhancing Verizon’s sustainability reporting and stake- holder engagement on ESG factors that align with Verizon’s core business strategy. The Senior Vice President and Chief ESG Officer heads a newly formed cross-functional team that focuses on strategic areas, including climate change and sustainability reporting, and oversees efforts to deliver on Verizon’s ESG commitments. The Chief ESG Officer regularly provides our Board’s Corporate Governance and Policy Committee with updates on the Company’s ESG priorities, commitments, and reporting.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: The perceptions of the groups of stakeholders with whom we interact bring relevant inputs to the process of identifying our challenges and opportunities. Aiming to address perceptions, a review of the relevance of the categories of stakeholders was made in 2020 – board of directors, shareholders/investors, customers, entities affiliated, government, suppliers, employees, press/media, regulatory bodies, representatives of civil society, sustainability experts and competitors, from the perspective of the Strategic Objectives (2021-2025 Strategic Map) and the results of the process carried out in 2016.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Score: 8/10\n"
     ]
    }
   ],
   "source": [
    "result = bert_trainer.predict(tokenized_ds[\"test\"])\n",
    "score = 0\n",
    "\n",
    "for i in range(PREDICT_SIZE):\n",
    "    groundtruth = np.argmax(result[0][i])\n",
    "    pred = tokenized_ds['test']['label'][i]\n",
    "\n",
    "    if (groundtruth == pred):\n",
    "        score += 1\n",
    "\n",
    "    print(f\"Text: {tokenized_ds['test']['text'][i]}\")\n",
    "    print(f\"Groundtruth: {groundtruth}\")\n",
    "    print(f\"Prediction: {pred}\\n\")\n",
    "\n",
    "print(f\"Score: {score}/{PREDICT_SIZE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
