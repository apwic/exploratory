{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import datasets\n",
    "import evaluate\n",
    "import accelerate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"climatebert/climate_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Based for Text Classification\n",
    "\n",
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = dataset[\"train\"][\"text\"]\n",
    "label_train = dataset[\"train\"][\"label\"]\n",
    "\n",
    "text_test = dataset[\"test\"][\"text\"]\n",
    "label_test = dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "_, train_set = dataset[\"train\"].train_test_split(test_size=TEST_SIZE).values()\n",
    "_, test_set = dataset[\"test\"].train_test_split(test_size=TEST_SIZE).values()\n",
    "\n",
    "mini_ds = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": train_set,\n",
    "        \"test\" : test_set\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 64\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets for RNN-Based Models (to compare with BERT-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max words for the vocabulary\n",
    "MAX_WORDS = 10000\n",
    "tokenizer_train = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# fit dataset to tokenizer\n",
    "tokenizer_train.fit_on_texts(text_train)\n",
    "tokenizer_test.fit_on_texts(text_test)\n",
    "\n",
    "# convert dataset to sequence of integer\n",
    "seq_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer_test.texts_to_sequences(text_test)\n",
    "\n",
    "# pad the sequence to fixed_length, will adjust later\n",
    "MAX_SEQ = 250\n",
    "X_train = pad_sequences(sequences=seq_train, maxlen=MAX_SEQ)\n",
    "X_test = pad_sequences(sequences=seq_test, maxlen=MAX_SEQ)\n",
    "\n",
    "# turn the lables into categorical\n",
    "y_train = to_categorical(label_train, 3)\n",
    "y_test = to_categorical(label_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset for the BERT-Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a9a24136ed4792940294fdfcdce98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1df5e8d6e84c0198063923a9a4d431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create tokenizer from BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(ds):\n",
    "    return tokenizer(ds[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized_ds = mini_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RNN-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "w2v = Word2Vec(sentences=text_train, vector_size=EMBEDDING_DIM, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_train.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in w2v.wv:\n",
    "            embedding_matrix[i] = w2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn_w2v = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# change the weight to embedding_matrix from Word2Vec\n",
    "rnn_w2v.add(Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=True))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn_w2v.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# Bidirectional LSTM layers with dropout and recurrent dropout\n",
    "rnn_w2v.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "rnn_w2v.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "# Batch normalization layer\n",
    "rnn_w2v.add(BatchNormalization())\n",
    "\n",
    "# Dense layer with ReLU activation\n",
    "rnn_w2v.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Dropout layer\n",
    "rnn_w2v.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn_w2v.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# Learning rate reduction on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Compile the RNN model with custom optimizer (Adam) and loss function (categorical_crossentropy)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "rnn_w2v.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 17s 423ms/step - loss: 1.0925 - accuracy: 0.4110 - val_loss: 1.0751 - val_accuracy: 0.5094\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 0.8525 - accuracy: 0.6280 - val_loss: 1.0751 - val_accuracy: 0.4656\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 13s 408ms/step - loss: 0.5024 - accuracy: 0.8120 - val_loss: 1.1118 - val_accuracy: 0.4469\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 0.2342 - accuracy: 0.9310 - val_loss: 1.1245 - val_accuracy: 0.4594\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 20s 638ms/step - loss: 0.1359 - accuracy: 0.9650 - val_loss: 1.2585 - val_accuracy: 0.4656\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 20s 631ms/step - loss: 0.0865 - accuracy: 0.9740 - val_loss: 1.3705 - val_accuracy: 0.4031\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 19s 591ms/step - loss: 0.0680 - accuracy: 0.9790 - val_loss: 1.9884 - val_accuracy: 0.3469\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 21s 654ms/step - loss: 0.0902 - accuracy: 0.9800 - val_loss: 1.8203 - val_accuracy: 0.3719\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 22s 681ms/step - loss: 0.0432 - accuracy: 0.9900 - val_loss: 1.9316 - val_accuracy: 0.3594\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 22s 687ms/step - loss: 0.0233 - accuracy: 0.9960 - val_loss: 2.3917 - val_accuracy: 0.3562\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 20s 615ms/step - loss: 0.0180 - accuracy: 0.9960 - val_loss: 2.6019 - val_accuracy: 0.3469\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 22s 674ms/step - loss: 0.0328 - accuracy: 0.9930 - val_loss: 2.5232 - val_accuracy: 0.3844\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 19s 580ms/step - loss: 0.0342 - accuracy: 0.9910 - val_loss: 3.1279 - val_accuracy: 0.4125\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 19s 610ms/step - loss: 0.0418 - accuracy: 0.9900 - val_loss: 6.5602 - val_accuracy: 0.3156\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 19s 583ms/step - loss: 0.0816 - accuracy: 0.9760 - val_loss: 2.8073 - val_accuracy: 0.2781\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 20s 618ms/step - loss: 0.0508 - accuracy: 0.9870 - val_loss: 2.8617 - val_accuracy: 0.4031\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 20s 609ms/step - loss: 0.0671 - accuracy: 0.9740 - val_loss: 3.1947 - val_accuracy: 0.3344\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 20s 634ms/step - loss: 0.0513 - accuracy: 0.9890 - val_loss: 3.4249 - val_accuracy: 0.3781\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 19s 586ms/step - loss: 0.0496 - accuracy: 0.9870 - val_loss: 3.3465 - val_accuracy: 0.3719\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 19s 581ms/step - loss: 0.0344 - accuracy: 0.9890 - val_loss: 3.5275 - val_accuracy: 0.3281\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_w2v_history = rnn_w2v.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 60ms/step - loss: 3.5275 - accuracy: 0.3281\n",
      "Loss:\t3.5275\n",
      "Accuracy:\t0.3281\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BERT-Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=pred, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_training_args = TrainingArguments(output_dir=\"trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    model= bert_model,\n",
    "    args= bert_training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b354e302e2c945ea9acb865202cd9281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cfbcf179c247858ac7d0d221e96b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7842980623245239, 'eval_accuracy': 0.75, 'eval_runtime': 12.167, 'eval_samples_per_second': 5.26, 'eval_steps_per_second': 0.658, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1def3e4dd934647b25db6c74be7cbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7379715442657471, 'eval_accuracy': 0.6875, 'eval_runtime': 5.7386, 'eval_samples_per_second': 11.153, 'eval_steps_per_second': 1.394, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692bbc27f96d49558832f7de5c58d830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7074193358421326, 'eval_accuracy': 0.71875, 'eval_runtime': 14.6874, 'eval_samples_per_second': 4.357, 'eval_steps_per_second': 0.545, 'epoch': 3.0}\n",
      "{'train_runtime': 307.3582, 'train_samples_per_second': 1.952, 'train_steps_per_second': 0.244, 'train_loss': 0.6102455647786459, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.6102455647786459, metrics={'train_runtime': 307.3582, 'train_samples_per_second': 1.952, 'train_steps_per_second': 0.244, 'train_loss': 0.6102455647786459, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95de31d09d2b49e6847b0d1b3594821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_result = bert_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7074193358421326,\n",
       " 'eval_accuracy': 0.71875,\n",
       " 'eval_runtime': 41.3466,\n",
       " 'eval_samples_per_second': 1.548,\n",
       " 'eval_steps_per_second': 0.193,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RNN-Based to BERT-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 932ms/step\n",
      "Text: sustainable strategy ‘red lines’ for our sustainable strategy range we incorporate a series of proprietary ‘red lines’ in order to ensure the poorest performing companies from an esg perspective are not eligible for investment\\Groundtruth: 0\n",
      "Predicted: 2\n",
      "\n",
      "Text: verizon’s environmental health and safety management system provides a framework for identifying controlling and reducing the risks associated with the environments in which we operate besides regular management system assessments internal and third party compliance audits and inspections are performed annually at hundreds of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions verizon’s environment health and safety efforts are directed and supported by experienced experts around the world that support our operations and facilities\\Groundtruth: 1\n",
      "Predicted: 0\n",
      "\n",
      "Text: in 2019 the company closed a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an affiliate of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cory cogeneration station to saskpower international and its 50 per cent ownership interest in brighton beach power to ontario power generation\\Groundtruth: 1\n",
      "Predicted: 1\n",
      "\n",
      "Text: in december 2020 the auc approved the electricity distribution and natural gas distribution requests to defer the compulsory distribution rate increases which would normally come into effect on january 1 2021 for both businesses the rate relief was requested to defer significant distribution rate increases which would be passed onto end use customers due to the formulaic approach of rate calculations under the auc pbr mechanism electricity distribution and natural gas distribution cited the current economic situation in alberta including the hardships faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to file an application by march 1 2021 outlining the duration of the rate freeze and collection timelines expected deferral values including carrying costs and anticipated impacts to customers\\Groundtruth: 0\n",
      "Predicted: 2\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions\\Groundtruth: 0\n",
      "Predicted: 1\n",
      "\n",
      "Score: 1/10\n"
     ]
    }
   ],
   "source": [
    "# using Word2Vec embedding\n",
    "predictions = rnn_w2v.predict(X_test[:PREDICT_SIZE])\n",
    "score = 0\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    pred = pred.index(max(pred))\n",
    "\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    groundtruth = groundtruth.index(max(groundtruth))\n",
    "\n",
    "    if (groundtruth == pred):\n",
    "        score += 1\n",
    "\n",
    "    print(f\"Text: {text}\\Groundtruth: {groundtruth}\\nPredicted: {pred}\\n\")\n",
    "\n",
    "print(f\"Score: {score}/{PREDICT_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb329c79aad4631b20e5a5ad33ee627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We recognize the importance of effectively managing climate-related risks and opportunities and have embedded them into Verizon’s existing processes and decision making. That is why we are committed to strengthening our existing Board-level oversight and governance structures with regard to climate. This includes clarifying lines of communication between Verizon man- agement and the Board on climate- and ESG-related issues, generally creating continuous and frequent lines of communication on sustainability issues, and continuing to elevate the transparency of our ESG disclosures.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: The objectives of the CCWG, which reflect the risks and opportunities facing our business from climate change, are: • Build operational climate resilience to protect Newton’s own assets • Effectively assess, integrate, monitor and manage climate-related investment risks and opportunities to protect and grow our clients’ assets, supported by our climate change investment focus group • Publish high-quality climate-related reports for clients, regulators and the public.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: We have streamlined our Crisis Response and Emergency Preparedness systems, and we continuously improve our ability to rapidly mobilize and effectively respond to crises globally. We incorporate learnings from responding to extreme weather events which enables us to continue to strengthen our emergency response capabilities.\n",
      "Groundtruth: 1\n",
      "Prediction: 0\n",
      "\n",
      "Text: (and the company may not be able to offset such impact, including, for example, through higher freight rates). Climate change legislation and regulation could also affect CN’s customers; make it difficult for CN’s customers to produce products in a cost-competitive manner due to increased energy costs; and increase legal costs related to defending and resolving legal claims and other litigation related to climate change.\n",
      "Groundtruth: 0\n",
      "Prediction: 0\n",
      "\n",
      "Text:  In October 2017, wildfires started to burn in California causing damage to many properties and spreading rapidly every day. As numerous fires continued to spread, a second wave of wildfires commenced in December thereby strengthening the severity of California’s weather conditions in Q4-2017. The fires have forced many people to be evacuated from their homes and neighbourhoods and have continued to burn in the early weeks of 2018.\n",
      "Groundtruth: 1\n",
      "Prediction: 0\n",
      "\n",
      "Text: Our Alignment with the UN SDGs As a global citizen, LG Chem is making efforts to contribute to the achievement of UN SDGs. Considering the direct and indirect impacts on our business models, we focus on the SDGs relevant to our businesses and engages in a variety of sustainability activities on specific targets.\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: Finally, there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change. This could possibly make Eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions.\n",
      "Groundtruth: 0\n",
      "Prediction: 0\n",
      "\n",
      "Text: While we seek to partner with organizations that mitigate their business risks associated with climate change, we recognize that there are inherent risks wherever business is conducted. Access to clean water and reliable energy in the communities where we conduct our business, whether for our offices or for our vendors, is a priority. Our major sites in California, Utah and India are vulnerable to prolonged droughts due to climate change. In the event of a natural disaster that disrupts business due to limited access to these resources, we have the potential to experience losses to our business, and added costs to resume operations. To accurately assess and take potential proactive action as appropriate, Adobe is aligned with the guidelines of the Financial Stability Board’s (“FSB”) Task Force on Climate-related Financial Disclosures (“TCFD”) recommendations.\n",
      "Groundtruth: 0\n",
      "Prediction: 0\n",
      "\n",
      "Text: As mentioned at the opening, this paper’s purpose is to support and supplement EON’s annual climate related disclosures and to provide a more detailed overview of EON’s transition to the net-zero carbon world It also aims to give readers a better understanding of EON’s annual climate-related facts and figures, to contextualize this information, and to provide more detailed examples of impacts\n",
      "Groundtruth: 1\n",
      "Prediction: 1\n",
      "\n",
      "Text: � SASB materiality: in the 2019 process, as a further element of analysis, account was taken of the materiality assessment of the 28 mapped themes obtained using the materiality assessment tool developed by the Sustainability Accounting Standards Board, with reference to a number of sectors, specifically including road and air transport, engineering, construction and logistics. SASB standards[3] specifically focus on how the pertinent themes are perceived by investors. Therefore, this materiality model is mainly based on an assessment of the effects ESG issues can have on the financial conditions or operating performance of companies within a specific sector.\n",
      "Groundtruth: 0\n",
      "Prediction: 1\n",
      "\n",
      "Score: 7/10\n"
     ]
    }
   ],
   "source": [
    "result = bert_trainer.predict(tokenized_ds[\"test\"])\n",
    "score = 0\n",
    "\n",
    "for i in range(PREDICT_SIZE):\n",
    "    groundtruth = np.argmax(result[0][i])\n",
    "    pred = tokenized_ds['test']['label'][i]\n",
    "\n",
    "    if (groundtruth == pred):\n",
    "        score += 1\n",
    "\n",
    "    print(f\"Text: {tokenized_ds['test']['text'][i]}\")\n",
    "    print(f\"Groundtruth: {groundtruth}\")\n",
    "    print(f\"Prediction: {pred}\\n\")\n",
    "\n",
    "print(f\"Score: {score}/{PREDICT_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
