{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"climatebert/climate_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "pd_test = pd.DataFrame.from_dict(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>− Scope 3: Optional scope that includes indire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Group is not aware of any noise pollution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global climate change could exacerbate certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setting an investment horizon is part and parc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate change the physical impacts of climate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Greenhouse gas Mitigation Measures Our five ye...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>We have updated our external sector statements...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>STOREBRAND'S USE Task Force on Climate-related...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Estimations of nanced emissions indicate the i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Emissions of CH4, which account for approximat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    − Scope 3: Optional scope that includes indire...      1\n",
       "1    The Group is not aware of any noise pollution ...      0\n",
       "2    Global climate change could exacerbate certain...      0\n",
       "3    Setting an investment horizon is part and parc...      0\n",
       "4    Climate change the physical impacts of climate...      0\n",
       "..                                                 ...    ...\n",
       "995  Greenhouse gas Mitigation Measures Our five ye...      1\n",
       "996  We have updated our external sector statements...      1\n",
       "997  STOREBRAND'S USE Task Force on Climate-related...      0\n",
       "998  Estimations of nanced emissions indicate the i...      1\n",
       "999  Emissions of CH4, which account for approximat...      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text, replacing symbol into space, remove symbols, and using stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    \n",
    "    return text\n",
    "\n",
    "pd_train['text'] = pd_train['text'].apply(clean_text)\n",
    "pd_train['text'] = pd_train['text'].str.replace('\\d+', '')\n",
    "\n",
    "pd_test['text'] = pd_test['text'].apply(clean_text)\n",
    "pd_test['text'] = pd_test['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = pd_train[\"text\"].values\n",
    "label_train = pd_train[\"label\"].values\n",
    "\n",
    "text_test = pd_test[\"text\"].values\n",
    "label_test = pd_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max words for the vocabulary\n",
    "MAX_WORDS = 50000\n",
    "tokenizer_train = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# fit dataset to tokenizer\n",
    "tokenizer_train.fit_on_texts(text_train)\n",
    "tokenizer_test.fit_on_texts(text_test)\n",
    "\n",
    "# convert dataset to sequence of integer\n",
    "seq_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer_test.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequence to fixed_length, will adjust later\n",
    "MAX_SEQ = 100\n",
    "X_train = pad_sequences(sequences=seq_train, maxlen=MAX_SEQ)\n",
    "X_test = pad_sequences(sequences=seq_test, maxlen=MAX_SEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the lables into categorical\n",
    "y_train = to_categorical(label_train, 3)\n",
    "y_test = to_categorical(label_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets already split, so will use validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN Model using Randomly Initialized Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# weights not assigned, will use randmoly initialized\n",
    "rnn.add(Embedding(input_dim=MAX_WORDS, output_dim=100, input_length=X_train.shape[1]))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 11s 191ms/step - loss: 0.0082 - accuracy: 0.9989 - val_loss: 1.3784 - val_accuracy: 0.7200\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 11s 192ms/step - loss: 0.0080 - accuracy: 0.9989 - val_loss: 1.3926 - val_accuracy: 0.7300\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 12s 201ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.4123 - val_accuracy: 0.7400\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 11s 197ms/step - loss: 0.0192 - accuracy: 0.9956 - val_loss: 1.3242 - val_accuracy: 0.7700\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 11s 185ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 1.3420 - val_accuracy: 0.7500\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 11s 196ms/step - loss: 0.0094 - accuracy: 0.9989 - val_loss: 1.4860 - val_accuracy: 0.6900\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 12s 217ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 1.3183 - val_accuracy: 0.7100\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 11s 196ms/step - loss: 0.0079 - accuracy: 0.9989 - val_loss: 1.3703 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_history = rnn.fit(X_train, y_train, epochs=8, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 53ms/step - loss: 1.3703 - accuracy: 0.7000\n",
      "Loss:\t1.3703\n",
      "Accuracy:\t0.7000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn.evaluate(X_val, y_val)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 42ms/step - loss: 3.4570 - accuracy: 0.3625\n",
      "Loss:\t3.4570\n",
      "Accuracy:\t0.3625\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is \"meh\", when using test data. Already tried to adjust the layers used, but still couldn't find models that produce better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN models using Word2Vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding matrix using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=text_train, vector_size=100, window=5, min_count=1, sg=0)\n",
    "w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, 100))\n",
    "for word, i in tokenizer_train.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in w2v.wv:\n",
    "            embedding_matrix[i] = w2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn_w2v = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# change the weight to embedding_matrix from Word2Vec\n",
    "rnn_w2v.add(Embedding(input_dim=MAX_WORDS, output_dim=100, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=True))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn_w2v.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn_w2v.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn_w2v.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn_w2v.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "57/57 [==============================] - 10s 146ms/step - loss: 0.0877 - accuracy: 0.9789 - val_loss: 0.6985 - val_accuracy: 0.7700\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 9s 159ms/step - loss: 0.0732 - accuracy: 0.9911 - val_loss: 1.0798 - val_accuracy: 0.7200\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 11s 185ms/step - loss: 0.0151 - accuracy: 0.9967 - val_loss: 1.1099 - val_accuracy: 0.7100\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 11s 188ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 1.2076 - val_accuracy: 0.7300\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 11s 186ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 1.3880 - val_accuracy: 0.7000\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 11s 185ms/step - loss: 0.0087 - accuracy: 0.9989 - val_loss: 1.4805 - val_accuracy: 0.6900\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 11s 192ms/step - loss: 0.0083 - accuracy: 0.9989 - val_loss: 1.5154 - val_accuracy: 0.6900\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 11s 190ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 1.6685 - val_accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_w2v_history = rnn_w2v.fit(X_train, y_train, epochs=8, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 37ms/step - loss: 1.3703 - accuracy: 0.7000\n",
      "Loss:\t1.3703\n",
      "Accuracy:\t0.7000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn.evaluate(X_val, y_val)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 42ms/step - loss: 3.5895 - accuracy: 0.4406\n",
      "Loss:\t3.5895\n",
      "Accuracy:\t0.4406\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is a bit better compared to randomly initialized, but still not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the models using two embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Text: sustainable strategy red lines sustainable strategy range incorporate series proprietary red lines order ensure poorest performing companies esg perspective eligible investment\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: verizons environmental health safety management system provides framework identifying controlling reducing risks associated environments operate besides regular management system assessments internal thirdparty compliance audits inspections performed annually hundreds facilities worldwide goal assessments identify correct sitespecific issues educate empower facility managers supervisors implement corrective actions verizons environment health safety efforts directed supported eperienced eperts around world support operations facilities\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: 2019 company closed series transactions related sale canadian fossil fuelbased electricity generation business transaction heartland generation ltd affiliate energy capital partners included sale 10 partly fully owned natural gasfired coalfired electricity generation assets located alberta british columbia two separate transactions company sold 50 per cent ownership interest cory cogeneration station saskpower international 50 per cent ownership interest brighton beach power ontario power generation\n",
      "Predicted: 2\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: december 2020 auc approved electricity distribution natural gas distribution requests defer compulsory distribution rate increases would normally come effect january 1 2021 businesses rate relief requested defer significant distribution rate increases would passed onto end use customers due formulaic approach rate calculations auc pbr mechanism electricity distribution natural gas distribution cited current economic situation alberta including hardships faced end use customers due covid19 pandemic rationale proceed interim rates electricity distribution natural gas distribution file application march 1 2021 outlining duration rate freeze collection timelines epected deferral values including carrying costs anticipated impacts customers\n",
      "Predicted: 2\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: finally reputational risk linked possibility oil companies may perceived institutions general public entities mainly responsible climate change could possibly make enis shares less attractive investment funds individual investors assess risk profile companies environmental social footprint making investment decisions\n",
      "Predicted: 0\n",
      "Groundtruth: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using randomly initialized embedding\n",
    "predictions = rnn.predict(X_test[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    print(f\"Text: {text}\\nPredicted: {pred.index(max(pred))}\\nGroundtruth: {groundtruth.index(max(groundtruth))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Text: sustainable strategy red lines sustainable strategy range incorporate series proprietary red lines order ensure poorest performing companies esg perspective eligible investment\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: verizons environmental health safety management system provides framework identifying controlling reducing risks associated environments operate besides regular management system assessments internal thirdparty compliance audits inspections performed annually hundreds facilities worldwide goal assessments identify correct sitespecific issues educate empower facility managers supervisors implement corrective actions verizons environment health safety efforts directed supported eperienced eperts around world support operations facilities\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: 2019 company closed series transactions related sale canadian fossil fuelbased electricity generation business transaction heartland generation ltd affiliate energy capital partners included sale 10 partly fully owned natural gasfired coalfired electricity generation assets located alberta british columbia two separate transactions company sold 50 per cent ownership interest cory cogeneration station saskpower international 50 per cent ownership interest brighton beach power ontario power generation\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: december 2020 auc approved electricity distribution natural gas distribution requests defer compulsory distribution rate increases would normally come effect january 1 2021 businesses rate relief requested defer significant distribution rate increases would passed onto end use customers due formulaic approach rate calculations auc pbr mechanism electricity distribution natural gas distribution cited current economic situation alberta including hardships faced end use customers due covid19 pandemic rationale proceed interim rates electricity distribution natural gas distribution file application march 1 2021 outlining duration rate freeze collection timelines epected deferral values including carrying costs anticipated impacts customers\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: finally reputational risk linked possibility oil companies may perceived institutions general public entities mainly responsible climate change could possibly make enis shares less attractive investment funds individual investors assess risk profile companies environmental social footprint making investment decisions\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Word2Vec embedding\n",
    "predictions = rnn_w2v.predict(X_test[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    print(f\"Text: {text}\\nPredicted: {pred.index(max(pred))}\\nGroundtruth: {groundtruth.index(max(groundtruth))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While randomly initialized embedding produce worst result than Word2Vector, both of the embedding couldn't produce a good enough results.\n",
    "\n",
    "### Randomly Initalized\n",
    "Accuracy: 36.25%\n",
    "\n",
    "### Word2Vec\n",
    "Accuracy: 44.06%\n",
    "\n",
    "\n",
    "_*both accuracy using different datasets than training._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
