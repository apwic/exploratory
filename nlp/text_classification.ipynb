{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"climatebert/climate_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "pd_test = pd.DataFrame.from_dict(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>− Scope 3: Optional scope that includes indire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Group is not aware of any noise pollution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global climate change could exacerbate certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setting an investment horizon is part and parc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate change the physical impacts of climate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Greenhouse gas Mitigation Measures Our five ye...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>We have updated our external sector statements...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>STOREBRAND'S USE Task Force on Climate-related...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Estimations of nanced emissions indicate the i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Emissions of CH4, which account for approximat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    − Scope 3: Optional scope that includes indire...      1\n",
       "1    The Group is not aware of any noise pollution ...      0\n",
       "2    Global climate change could exacerbate certain...      0\n",
       "3    Setting an investment horizon is part and parc...      0\n",
       "4    Climate change the physical impacts of climate...      0\n",
       "..                                                 ...    ...\n",
       "995  Greenhouse gas Mitigation Measures Our five ye...      1\n",
       "996  We have updated our external sector statements...      1\n",
       "997  STOREBRAND'S USE Task Force on Climate-related...      0\n",
       "998  Estimations of nanced emissions indicate the i...      1\n",
       "999  Emissions of CH4, which account for approximat...      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    \n",
    "    return text\n",
    "\n",
    "pd_train['text'] = pd_train['text'].apply(clean_text)\n",
    "pd_train['text'] = pd_train['text'].str.replace('\\d+', '')\n",
    "\n",
    "pd_test['text'] = pd_test['text'].apply(clean_text)\n",
    "pd_test['text'] = pd_test['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = pd_train[\"text\"].values\n",
    "label_train = pd_train[\"label\"].values\n",
    "\n",
    "text_test = pd_test[\"text\"].values\n",
    "label_test = pd_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max words for the vocabulary\n",
    "MAX_WORDS = 50000\n",
    "tokenizer_train = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# fit dataset to tokenizer\n",
    "tokenizer_train.fit_on_texts(text_train)\n",
    "tokenizer_test.fit_on_texts(text_test)\n",
    "\n",
    "# convert dataset to sequence of integer\n",
    "seq_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer_test.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequence to fixed_length, will adjust later\n",
    "MAX_SEQ = 100\n",
    "X_train = pad_sequences(sequences=seq_train, maxlen=MAX_SEQ)\n",
    "X_test = pad_sequences(sequences=seq_test, maxlen=MAX_SEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(label_train, 3)\n",
    "y_test = to_categorical(label_test, 3)\n",
    "# y_train = label_train\n",
    "# y_test = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN Model using Randomly Initialized Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "rnn.add(Embedding(input_dim=MAX_WORDS, output_dim=100, input_length=X_train.shape[1]))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation\n",
    "rnn.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.2129 - val_accuracy: 0.7400\n",
      "Epoch 2/8\n",
      "9/9 [==============================] - 4s 495ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2230 - val_accuracy: 0.7400\n",
      "Epoch 3/8\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2290 - val_accuracy: 0.7400\n",
      "Epoch 4/8\n",
      "9/9 [==============================] - 4s 492ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 1.2254 - val_accuracy: 0.7400\n",
      "Epoch 5/8\n",
      "9/9 [==============================] - 4s 494ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 1.2239 - val_accuracy: 0.7300\n",
      "Epoch 6/8\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 1.2313 - val_accuracy: 0.7400\n",
      "Epoch 7/8\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.2406 - val_accuracy: 0.7400\n",
      "Epoch 8/8\n",
      "9/9 [==============================] - 4s 490ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.2482 - val_accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_history = rnn.fit(X_train, y_train, epochs=8, batch_size=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 51ms/step - loss: 3.0449 - accuracy: 0.3781\n",
      "Loss:\t3.0449\n",
      "Accuracy:\t0.3781\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN models using Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=text_train, vector_size=128, window=5, min_count=1, sg=0)\n",
    "w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, 128))\n",
    "for word, i in tokenizer_train.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in w2v.wv:\n",
    "            embedding_matrix[i] = w2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn_w2v = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# change the weight to embedding_matrix from Word2Vec\n",
    "rnn_w2v.add(Embedding(input_dim=MAX_WORDS, output_dim=100, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=True))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn_w2v.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn_w2v.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn_w2v.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation\n",
    "rnn_w2v.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 7s 147ms/step - loss: 1.0639 - accuracy: 0.4156 - val_loss: 0.9764 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 4s 132ms/step - loss: 0.6529 - accuracy: 0.7500 - val_loss: 0.5949 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.1746 - accuracy: 0.9611 - val_loss: 0.8504 - val_accuracy: 0.7400\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.1169 - accuracy: 0.9711 - val_loss: 0.8386 - val_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 5s 158ms/step - loss: 0.0610 - accuracy: 0.9889 - val_loss: 0.9804 - val_accuracy: 0.7300\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 5s 171ms/step - loss: 0.0370 - accuracy: 0.9911 - val_loss: 0.9632 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 5s 157ms/step - loss: 0.0189 - accuracy: 0.9978 - val_loss: 1.0125 - val_accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0159 - accuracy: 0.9978 - val_loss: 1.0873 - val_accuracy: 0.7700\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 5s 174ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 1.1634 - val_accuracy: 0.7200\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 5s 168ms/step - loss: 0.0143 - accuracy: 0.9978 - val_loss: 1.0771 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_w2v_history = rnn_w2v.fit(X_train, y_train, epochs=8, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 22ms/step - loss: 3.3066 - accuracy: 0.3531\n",
      "Loss:\t3.3066\n",
      "Accuracy:\t0.3531\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
