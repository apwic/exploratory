{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"climatebert/climate_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "pd_test = pd.DataFrame.from_dict(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>− Scope 3: Optional scope that includes indire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Group is not aware of any noise pollution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global climate change could exacerbate certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setting an investment horizon is part and parc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate change the physical impacts of climate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Greenhouse gas Mitigation Measures Our five ye...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>We have updated our external sector statements...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>STOREBRAND'S USE Task Force on Climate-related...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Estimations of nanced emissions indicate the i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Emissions of CH4, which account for approximat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    − Scope 3: Optional scope that includes indire...      1\n",
       "1    The Group is not aware of any noise pollution ...      0\n",
       "2    Global climate change could exacerbate certain...      0\n",
       "3    Setting an investment horizon is part and parc...      0\n",
       "4    Climate change the physical impacts of climate...      0\n",
       "..                                                 ...    ...\n",
       "995  Greenhouse gas Mitigation Measures Our five ye...      1\n",
       "996  We have updated our external sector statements...      1\n",
       "997  STOREBRAND'S USE Task Force on Climate-related...      0\n",
       "998  Estimations of nanced emissions indicate the i...      1\n",
       "999  Emissions of CH4, which account for approximat...      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using RNN with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text, replacing symbol into space, remove symbols, and using stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    \n",
    "    return text\n",
    "\n",
    "pd_train['text'] = pd_train['text'].apply(clean_text)\n",
    "pd_train['text'] = pd_train['text'].str.replace('\\d+', '')\n",
    "\n",
    "pd_test['text'] = pd_test['text'].apply(clean_text)\n",
    "pd_test['text'] = pd_test['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = pd_train[\"text\"].values\n",
    "label_train = pd_train[\"label\"].values\n",
    "\n",
    "text_test = pd_test[\"text\"].values\n",
    "label_test = pd_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max words for the vocabulary\n",
    "MAX_WORDS = 50000\n",
    "tokenizer_train = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# fit dataset to tokenizer\n",
    "tokenizer_train.fit_on_texts(text_train)\n",
    "tokenizer_test.fit_on_texts(text_test)\n",
    "\n",
    "# convert dataset to sequence of integer\n",
    "seq_train = tokenizer_train.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer_test.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequence to fixed_length, will adjust later\n",
    "MAX_SEQ = 100\n",
    "X_train = pad_sequences(sequences=seq_train, maxlen=MAX_SEQ)\n",
    "X_test = pad_sequences(sequences=seq_test, maxlen=MAX_SEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the lables into categorical\n",
    "y_train = to_categorical(label_train, 3)\n",
    "y_test = to_categorical(label_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets already split, so will use validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN Model using Randomly Initialized Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# weights not assigned, will use randmoly initialized\n",
    "rnn.add(Embedding(input_dim=MAX_WORDS, output_dim=100, input_length=X_train.shape[1]))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "57/57 [==============================] - 12s 170ms/step - loss: 0.9870 - accuracy: 0.5000 - val_loss: 0.6408 - val_accuracy: 0.6900\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 10s 184ms/step - loss: 0.5808 - accuracy: 0.7078 - val_loss: 0.7276 - val_accuracy: 0.7300\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 12s 207ms/step - loss: 0.3123 - accuracy: 0.8922 - val_loss: 0.8825 - val_accuracy: 0.7300\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 12s 208ms/step - loss: 0.1256 - accuracy: 0.9667 - val_loss: 0.8873 - val_accuracy: 0.7100\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 11s 199ms/step - loss: 0.0541 - accuracy: 0.9889 - val_loss: 1.0822 - val_accuracy: 0.7100\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 11s 197ms/step - loss: 0.0356 - accuracy: 0.9922 - val_loss: 1.2633 - val_accuracy: 0.7000\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 11s 196ms/step - loss: 0.0269 - accuracy: 0.9944 - val_loss: 1.2337 - val_accuracy: 0.7400\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 11s 200ms/step - loss: 0.0299 - accuracy: 0.9911 - val_loss: 1.1081 - val_accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_history = rnn.fit(X_train, y_train, epochs=8, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 47ms/step - loss: 1.1081 - accuracy: 0.7200\n",
      "Loss:\t1.1081\n",
      "Accuracy:\t0.7200\n"
     ]
    }
   ],
   "source": [
    "# evaluate using data validation\n",
    "loss, accuracy = rnn.evaluate(X_val, y_val)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 58ms/step - loss: 2.4801 - accuracy: 0.3688\n",
      "Loss:\t2.4801\n",
      "Accuracy:\t0.3688\n"
     ]
    }
   ],
   "source": [
    "# evaluate using data test\n",
    "loss, accuracy = rnn.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is \"meh\", when using test data. Already tried to adjust the layers used, but still couldn't find models that produce better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN models using Word2Vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding matrix using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "w2v = Word2Vec(sentences=text_train, vector_size=EMBEDDING_DIM, window=5, min_count=1, sg=0)\n",
    "w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_train.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in w2v.wv:\n",
    "            embedding_matrix[i] = w2v.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequential model to stack layers\n",
    "rnn_w2v = Sequential()\n",
    "\n",
    "# embedding layer to convert integer tokens into dense vectors\n",
    "# change the weight to embedding_matrix from Word2Vec\n",
    "rnn_w2v.add(Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=True))\n",
    "\n",
    "# performs variational dropout in NLP models\n",
    "rnn_w2v.add(SpatialDropout1D(rate=0.2))\n",
    "\n",
    "# bidirectional with 100 unit\n",
    "# process sequence in both direction, it's said to capture context efficiently\n",
    "rnn_w2v.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# add final layer of 50 unit\n",
    "rnn_w2v.add(Bidirectional(LSTM(50)))\n",
    "\n",
    "# add dense layer, with 3 output and softmax activation (used for multiclass)\n",
    "rnn_w2v.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# compile the RNN model\n",
    "rnn_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "57/57 [==============================] - 13s 170ms/step - loss: 0.9626 - accuracy: 0.4822 - val_loss: 0.6399 - val_accuracy: 0.6900\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 9s 163ms/step - loss: 0.6099 - accuracy: 0.7300 - val_loss: 0.5884 - val_accuracy: 0.7300\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 11s 186ms/step - loss: 0.2944 - accuracy: 0.9022 - val_loss: 0.6771 - val_accuracy: 0.7200\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 11s 200ms/step - loss: 0.1002 - accuracy: 0.9733 - val_loss: 0.7919 - val_accuracy: 0.7400\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 11s 195ms/step - loss: 0.0694 - accuracy: 0.9844 - val_loss: 0.9679 - val_accuracy: 0.7400\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 11s 196ms/step - loss: 0.0178 - accuracy: 0.9989 - val_loss: 0.9812 - val_accuracy: 0.7500\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 11s 192ms/step - loss: 0.0133 - accuracy: 0.9989 - val_loss: 1.0911 - val_accuracy: 0.7500\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 11s 194ms/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 1.1746 - val_accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "rnn_w2v_history = rnn_w2v.fit(X_train, y_train, epochs=8, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 58ms/step - loss: 1.1746 - accuracy: 0.7400\n",
      "Loss:\t1.1746\n",
      "Accuracy:\t0.7400\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_val, y_val)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 54ms/step - loss: 3.1745 - accuracy: 0.4000\n",
      "Loss:\t3.1745\n",
      "Accuracy:\t0.4000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = rnn_w2v.evaluate(X_test, y_test)\n",
    "print(f\"Loss:\\t{loss:.4f}\")\n",
    "print(f\"Accuracy:\\t{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy between two models is not consistent which one is better. It really depends on the layers used, embedding dimension, and even batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the models using two embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 564ms/step\n",
      "Text: sustainable strategy red lines sustainable strategy range incorporate series proprietary red lines order ensure poorest performing companies esg perspective eligible investment\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: verizons environmental health safety management system provides framework identifying controlling reducing risks associated environments operate besides regular management system assessments internal thirdparty compliance audits inspections performed annually hundreds facilities worldwide goal assessments identify correct sitespecific issues educate empower facility managers supervisors implement corrective actions verizons environment health safety efforts directed supported eperienced eperts around world support operations facilities\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: 2019 company closed series transactions related sale canadian fossil fuelbased electricity generation business transaction heartland generation ltd affiliate energy capital partners included sale 10 partly fully owned natural gasfired coalfired electricity generation assets located alberta british columbia two separate transactions company sold 50 per cent ownership interest cory cogeneration station saskpower international 50 per cent ownership interest brighton beach power ontario power generation\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: december 2020 auc approved electricity distribution natural gas distribution requests defer compulsory distribution rate increases would normally come effect january 1 2021 businesses rate relief requested defer significant distribution rate increases would passed onto end use customers due formulaic approach rate calculations auc pbr mechanism electricity distribution natural gas distribution cited current economic situation alberta including hardships faced end use customers due covid19 pandemic rationale proceed interim rates electricity distribution natural gas distribution file application march 1 2021 outlining duration rate freeze collection timelines epected deferral values including carrying costs anticipated impacts customers\n",
      "Predicted: 2\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: finally reputational risk linked possibility oil companies may perceived institutions general public entities mainly responsible climate change could possibly make enis shares less attractive investment funds individual investors assess risk profile companies environmental social footprint making investment decisions\n",
      "Predicted: 0\n",
      "Groundtruth: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using randomly initialized embedding\n",
    "predictions = rnn.predict(X_test[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    print(f\"Text: {text}\\nPredicted: {pred.index(max(pred))}\\nGroundtruth: {groundtruth.index(max(groundtruth))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 464ms/step\n",
      "Text: sustainable strategy red lines sustainable strategy range incorporate series proprietary red lines order ensure poorest performing companies esg perspective eligible investment\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: verizons environmental health safety management system provides framework identifying controlling reducing risks associated environments operate besides regular management system assessments internal thirdparty compliance audits inspections performed annually hundreds facilities worldwide goal assessments identify correct sitespecific issues educate empower facility managers supervisors implement corrective actions verizons environment health safety efforts directed supported eperienced eperts around world support operations facilities\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: 2019 company closed series transactions related sale canadian fossil fuelbased electricity generation business transaction heartland generation ltd affiliate energy capital partners included sale 10 partly fully owned natural gasfired coalfired electricity generation assets located alberta british columbia two separate transactions company sold 50 per cent ownership interest cory cogeneration station saskpower international 50 per cent ownership interest brighton beach power ontario power generation\n",
      "Predicted: 1\n",
      "Groundtruth: 1\n",
      "\n",
      "Text: december 2020 auc approved electricity distribution natural gas distribution requests defer compulsory distribution rate increases would normally come effect january 1 2021 businesses rate relief requested defer significant distribution rate increases would passed onto end use customers due formulaic approach rate calculations auc pbr mechanism electricity distribution natural gas distribution cited current economic situation alberta including hardships faced end use customers due covid19 pandemic rationale proceed interim rates electricity distribution natural gas distribution file application march 1 2021 outlining duration rate freeze collection timelines epected deferral values including carrying costs anticipated impacts customers\n",
      "Predicted: 1\n",
      "Groundtruth: 0\n",
      "\n",
      "Text: finally reputational risk linked possibility oil companies may perceived institutions general public entities mainly responsible climate change could possibly make enis shares less attractive investment funds individual investors assess risk profile companies environmental social footprint making investment decisions\n",
      "Predicted: 0\n",
      "Groundtruth: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Word2Vec embedding\n",
    "predictions = rnn_w2v.predict(X_test[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer_test.sequences_to_texts(X_test), predictions, y_test[:5]):\n",
    "    pred = prediction.tolist()\n",
    "    groundtruth = groundtruth.tolist()\n",
    "    print(f\"Text: {text}\\nPredicted: {pred.index(max(pred))}\\nGroundtruth: {groundtruth.index(max(groundtruth))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Comparison\n",
    "\n",
    "In a comparative analysis of two embedding methods, randomly initialized embeddings and Word2Vec embeddings were evaluated for their performance. The results indicate that while Word2Vec embeddings outperform randomly initialized embeddings, both types of embeddings fall short of delivering satisfactory results.\n",
    "\n",
    "#### Randomly Initialized Embedding\n",
    "- Accuracy: 36.56%\n",
    "\n",
    "Randomly initialized embeddings achieved an accuracy of 36.56%. However, it's crucial to note that this accuracy may vary significantly based on several factors, including the number of layers, embedding dimension, batch size, and the number of training epochs used. The inconsistency in accuracy highlights the sensitivity of this approach to hyperparameter settings.\n",
    "\n",
    "#### Word2Vec Embedding\n",
    "- Accuracy: 40%\n",
    "\n",
    "On the other hand, Word2Vec embeddings performed relatively better with an accuracy of 40%. Similar to randomly initialized embeddings, the accuracy of Word2Vec embeddings is highly dependent on hyperparameter choices such as the number of layers, embedding dimension, batch size, and training epochs.\n",
    "\n",
    "It is important to emphasize that achieving a high level of accuracy with either embedding method is a complex task and requires careful tuning of these hyperparameters. The choice between randomly initialized embeddings and Word2Vec embeddings should be made considering the specific requirements and constraints of the problem at hand.\n",
    "\n",
    "Please note that these accuracy values are not absolute and can vary significantly based on the configuration of the models and data used in the experiments.\n",
    "\n",
    "\n",
    "_*both accuracy using different datasets than training._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
